지도학습
-의사결정 나무(Decision Trees)
데이터의 특징들을 기반으로
하나의 루트 노드(Root Node)에서 시작하여 분기(분할)하며,
각 분기마다 데이터를 가장 잘 분류할 수 있는 특징을 선택하여 트리를 구성.

각 노드는 특징(feature)/속성(attribute)을 나타내고,
가장 하위에 있는 노드는 결정 노드(decision node)/리프 노드(leaf node)라고
부르며,
분류 또는 예측 결과를 제공.

루트노드( 맨위) 리프노드(아래 가지 노드)

정보획득량(Information Gain)은 의사결정 나무에서 각 특징을 선택할 때 사용되는
척도. 정보획득량은 불순도(impurity)를 기반으로 계산

지니 불순도 (Gini Impurity)
•데이터의 불순도를 측정하는 지표로, 엔트로피(무질서한 정도)와 유사한 개념.
•의사결정나무는 지니 불순도를 최소화하는 방향으로 특징들을 선택하여 분기.
•지니 불순도가 작을수록 분류가 잘 이루어진다고 판단
•데이터의 불순도를 측정하는 지표로, 엔트로피와 유사한 개념.
•의사결정나무는 지니 불순도를 최소화하는 방향으로 특징들을 선택하여 분기.
•지니 불순도가 작을수록 분류가 잘 이루어진다고 판단

엔트로피
정보 이론에서 나온 개념, 데이터 집합의 불순도를 측정하는 지표.
•의사결정나무는 엔트로피를 최소화하는 방향으로 특징들을 선택하여 분기.
•엔트로피 값이 낮을수록 분류가 잘 이루어진다고 판단.
•leaf node로 갈수록 엔트로피(또는 지니 불순도)의 숫자가 대체로 낮아지나
높아지는 경우 :
데이터의 불확실성이 증가하고, 클래스 간의 경계가 모호해지는 현상.
이를 방지하기 위해 decision tree의 깊이를 제어하거나,
다른 알고리즘과의 조합 등의 전략을 사용할 수 있다.

엔트로피 또는 지니 불순도의 수치가
leaf node로 갈수록 엔트로피(또는 지니 불순도)의 숫자가 높아지는 경우 :
데이터의 불확실성이 증가하고, 클래스 간의 경계가 모호해지는 현상.
이를 방지하기 위해 decision tree의 깊이를 제어하거나,
다른 알고리즘과의 조합 등의 전략을 사용할 수 있다.

엔트로피 vs 지니불순도
엔트로피는 데이터 집합의 불확실성을 나타내는 지표.
데이터의 분포가 균일할수록 값이 최대가 되며, 한 클래스에 몰려있을수록 값이 최소.
엔트로피는 0 ~ 1 사이의 값, 0에 가까울수록 순수한 클래스로 구성된 데이터
지니 불순도는 CART(Classification and Regression Trees) 알고리즘에서 사용되는
불순도 지표. (다재다능한 트리라는 의미?)
데이터가 다양한 클래스에 고르게 분포되면 값이 최대,
특정 클래스에 몰릴수록 값이 최소.
지니 불순도는 0 ~ 1 사이의 값, 0에 가까울수록 순수한 클래스로 구성된 데이터

엔트로피 vs 지니불순도
지니 불순도는 클래스 비율의 제곱을 더하기 때문에, 값이 작은 클래스의 비율에
민감하게 반응. 따라서, 클래스의 크기가 크게 차이나는 경우에는 지니 불순도가
더 작은 값을 가질 가능성이 높다.
엔트로피는 클래스 비율의 로그 값을 사용하기 때문에, 클래스의 크기가
차이나는 경우에도 상대적으로 안정적인 값을 가진다. 따라서, 클래스의 크기가
크게 차이나는 경우에는 엔트로피를 사용하는 것이 더 적절할 수 있다.

정보 획득량(Information Gain)
• 엔트로피의 감소량을 측정하는 지표.
•각 분기마다 정보 이득을 최대화하는 특징을 선택하여 트리를 구성.
•정보 이득이 큰 특징은 데이터를 가장 잘 구분할 수 있는 특징이라고 판단.
지니 불순도를 구할 수 있다면, 불순도가 감소된 만큼
어떤 질문에서 얻을 수 있는 정보 획득량(Information Gain)을 산출가능.
예시) Information Gain = 0.4 - (0 + 0.15) = 0.25
이전 단계 불순도(0.4)에서 다음 단계의 불순도 합을 빼주면 정보 획득량이 된다.
분할된 데이터 세트들의 불순도가 작을수록 정보 획득량은 증가한다.

가중치(Weight)
지니 불순도와 정보 획득량(information gain)의 계산에 있어서
데이터세트의 크기로 가중치(weight)를 적용한다.
예시) Information Gain = 0.5 - ((0.5*0.2) + (0.48*(0.5) + (0.44*(0.3)) = 0.026
데이터 세트의 크기가 작을 수록 그 불순도의 영향력도 작아진다.

의사결정나무의 한계
여러 경우 중 하나의 질문을 먼저 결정해야 할 때마다
그 순간에 최적이라고 생각되는 것을 선택해 나가면서 최종적인 해답에 도달
더 좋은 트리를 만들 수 있는 가능성을 배제하게 되는 탐욕 알고리즘 가능성(GREEDY)
나무가 커질수록(의사결정의 질문이 많아질 수록)
학습 데이터에 알맞게 모델이 만들어져서 현실 데이터로 일반화(generalization)하기
어려워지는 경향이 있다.(Overfitting)

의사결정나무의 한계
Overfitting을 해결하려면 나무가지를 잘라내서 크기를 줄여야 할수도 있다.
(가지치기,pruning)

Decision Tree의 장점:
1.해석력: 분류나 회귀 문제에 대한 결과를 직관적으로 이해하기 쉽게 나타낼 수
있다. 트리의 분기와 조건에 따라 어떤 특성이 중요한지 파악할 수 있어서 모델의
해석이 용이.
2.비선형 관계 모델링: 트리 구조로 인해 변수들 간의 상호작용과 비선형 패턴을
모델링할 수 있어서 유연한 모델링이 가능.
3.특성 중요도 추정: 중요한 특성을 선택하여 모델을 개선하거나 특성
선택(feature selection)을 수행할 때 유용.
4.이상치와 누락된 값에 강건함: 트리의 구조는 이상치에 덜 민감하며, 누락된
값이 있는 데이터도 처리할 수 있다.

Decision Tree의 단점과 이슈:
1.과적합: 트리의 깊이가 깊어지면 모델이 훈련 데이터에 너무 맞춰져 다른 데이터에서
일반화 성능이 저하될 수 있다. 적절한 가지치기(pruning)나 규제 방법을 사용하여
과적합을 제어해야 .
2.데이터 불균형 처리: 데이터 불균형이 심한 경우 소수 클래스에 대한 학습이 잘
이루어지지 않을 수 있다. 가중치를 부여하는 등의 방법을 사용해야 할 수 있다.
3.계산복잡성: 트리의 깊이가 깊어질수록 모델의 크기가 커지고 예측 속도가 느려질 수
있다. 트리의 크기를 제어하는 하이퍼파라미터를 조정하거나 앙상블 기법을 사용하여
이러한 이슈를 완화할 수 있다.
4.데이터 변화에 민감함: 데이터의 작은 변화에도 모델이 크게 변할 수 있다.