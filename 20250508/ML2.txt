기술통계학(descriptive): 수집된 데이터 자체를 요약하고 설명하는 것에 초점
데이터의 중심 경향과 변동성을 파악하기 위한 기본 통계 개념
평균, 중간값, 분산, 표준편차 등을 통해 데이터의 특성을 이해할 수 있다.

확률 통계학(Inference):(Inferential): 수집된 데이터로 모집단(전체 데이터)을 추론 하는 것이 목적
데이터가 어뗗게 분포되어 있는지를 나타내며,
정규분포와 이항분포

기술통계학:
평균 (Mean): 데이터 세트의 모든 값의 합을 데이터의 개수로 나눈 값.
예시: 주어진 데이터 [2, 4, 6, 8, 10]의 평균은 (2+4+6+8+10)/5 = 6.
    머신 러닝 사용 예시:
    평균은 데이터의 중심 경향을 나타내며,
    데이터의 특성을 이해하고 이상치를 감지하는 데 사용됩니다.
중간값(Median): 데이터 세트를 크기 순으로 정렬했을 때 가운데 위치한 값.
    예시: 데이터 [2, 4, 6, 8, 10]의 중간값은 6입니다. 데이터 [2, 4, 6, 8, 10, 12]의
    중간값은 (6+8)/2 = 7.
        머신러닝 사용 예시: 중간값은 데이터의 분포를 이해하고 이상치에 영향을
        받지 않는 중심 경향을 파악하는 데 유용.

기술통계학:
분산(Variance): 데이터의 각 값이 평균으로부터 얼마나 떨어져 있는지
예시: 데이터 [2, 4, 6, 8, 10]의 분산은 [(2-6)² + (4-6)² + (6-6)² + (8-6)² + (10-6)²]/5 = 8.
머신러닝 사용 예시: 분산은 데이터의 변동성,
 모델의 성능 평가와 특성의 중요도 분석에 사용.
표준편차(Standard Deviation):
설명: 분산의 제곱근으로, 데이터의 분포를 나타내는 값.
예시: 데이터 [2, 4, 6, 8, 10]의 표준편차는 √8 ≈ 2.83입니다.
머신러닝 사용예시: 표준편차는 데이터의 변동성을 직관적으로 이해하는 데
사용되며, 데이터 전처리 및 정규화 과정에서 활용.

1. 얼마나 멀리 떨어져 있는지 (분산)
분산은 각 데이터 포인트가 평균으로부터 얼마나 멀리 떨어져 있는지를 제곱하여
평균화한 값입니다.
제곱을 하는 이유는 데이터가 평균보다 낮거나 높은 경우 모두 거리의 크기를 동일하게 고려하기 위함이다. 분산의 값이 크다는 것은 데이터 포인트들이
평균에서 멀리 떨어져 있다는 것을 의미하며, 데이터가 평균 주위에 밀집되어 있지
않다는 것을 나타냅니다.

2. 얼마나 퍼져 있는지(표준편차)
표준편차는 분산의 제곱근, 데이터가 평균 주위로 얼마나 퍼져 있는지를 보여줍니다.
표준편차가 클수록 데이터 포인트들은 평균에서 넓게 퍼져 있고, 표준편차가 작을수록
데이터 포인트들은 평균에 가깝게 밀집해 있습니다.
이 값은 원래 데이터와 같은 단위로 표현되므로 해석하기가 더 쉽습니다.

확률 통계학
정규분포(Normal Distribution):
설명: 데이터가 평균을 중심으로 대칭적으로 분포하는 종 모양의 분포.
 평균과 표준편차에 의해 정의됩니다.
• 연속확률분포(continuous).
예시: 사람의 키, 시험 점수 등 많은 자연 현상과 사회적 현상이 정규분포를 따릅니다.
자연 현상에서의 불확실성과 변동성 때문에 발생하며,
중앙에 몰려 있고 양쪽으로 갈수록 희박해지는 패턴을 보입니다.(중심극한 정리)
머신러닝 사용 예시:
정규분포는 가우시안 나이브 베이즈 분류기, 정규화, PCA 등 에 사용.
데이터가 정규분포를 따를 때,
평균을 중심으로 데이터의 대다수가 존재하며, 이를 통해 예측과 분석이 용이.

표준 정규 분포(Standard Normal Distribution)
기존 정규분포에서 평균을 빼고 표준편차로 나눈 결과의 분포로
평균이 0이고, 분산이 1인 정규 분포
관측값이 평균에서 몇배의 표준편차만큼 떨어져 분포하는지를 알 수 있다.
표준 정규분포로 데이터를 변환하는 이유는 모델이 더 빠르고 안정적으로 학습
StandardScaler() 떨어져있는 정도만 보는경우

고전 ML 모델은 정규분포를 가정함으로써
수학적 단순성과 통계적 타당성을 확보하며 예측을 “정량적으로” 해석할 수 있도록 설계
하지만 최근 딥러닝/트리 기반 모델은 이런 가정에 덜 의존하며,
데이터 분포가 복잡해도 잘 동작하도록 만들어져 있습니다.
정규분포를 가정한 모델은 데이터가 실제로 정규에 가깝다는 전제가 필요하지만,
대부분의 실제 데이터는 이를 만족하지 않음.
따라서 변환, 정규성 검정, 모델교체를 통해 이를 유연하게 보완 해야함

정규 분포를 따르지 않는 데이터를 처리하는 5가지 주요 보완 방법
방법 설명 예시
1. 변환 (Transformation) 정규분포에 가깝게 만드는 수학적 변환 로그, sqrt, Box-Cox, Yeo-Johnson
2. 이상치 제거 또는 완화 극단값 제거 또는 클리핑 Z-score 기반 필터링, IQR 범위 기준
3. 비모수 모델 사용 분포 가정을 하지 않는 모델로 전환 트리 계열 (Random Forest, XGBoost), SVM
4. 순위 기반 접근 수치형 값을 순서형 또는 등급형 처리 Rank encoding, Quantile binning
5. 정규성 검정 및 분포 시각화 재설계 Q-Q Plot, Shapiro-Wilk Test 등으로 정규성 확인 후 변수 개별 조정 일부 변수만 변환하고, 나머지는 그대로 사용

분포 이름 어원 · 뜻 쉬운 설명
정규 (Normal) "표준적인", "일반적인" 가장 많이 쓰는 종 모양 분포 (키, 성적 등)
표준정규 평균=0, 표준편차=1로 정규화된 것 정규분포를 가장 단순하게 만든 버전
지수 (Exponential) exponential = "지수 함수" 기반 다음 사건까지 걸리는 시간 (예: 고장날 때까지)
감마 (Gamma) 그리스 문자 γ에서 유래 지수분포 여러 개 합친 것
카이제곱 (Chi-square) χ²: 그리스 문자 ‘카이’ 분산(제곱합) 관련 검정에 사용
t-분포 학생(Student)이라는 필명 사용한 통계학자 표본 수 적을 때 평균 검정에 씀
베타 (Beta) 그리스 문자 β에서 유래 확률의 분포를 예측할 때 (0~1 사이 값)
균등 (Uniform) uniform = “한결같은” 모든 값이 나올 확률이 똑같음 (예: 주사위 1~6)

이항분포
이항분포(Binomial Distribution):
설명: 성공 또는 실패 두가지 경과만 가능한 실험을 여러 번 반복했을 때의 성공
횟수를 나타내는 분포.
• 이산 확률 분포 (discrete).
단순성과 적용 범위 때문에 이항분포가 더 자주 사용,
현실세계에는 다항분포보다 이항분포의 형태가 많이 존재
예시: 동전을 10번 던졌을 때 나올 수 있는 앞면의 횟수.
머신 러닝 사용에시: 이항분포는 분류 문제에서 클래스 레이블이 이항인 경우에
사용될 수 있으며, 로지스틱 회귀 등의 알고리즘에서 활용됩니다

n번의 독립적인 시도를 계속할 경우, 이항분포는 정규분포와 유사한 성공 확률

x축은 성공 횟수,y축은 확률 밀도
(동전을 20번 던졌을 때 앞면(H)이 나오는 횟수의 확률분포 그래프를 가정시
대칭적인 종 모양을 가지며, 가운데 10H 부근에서 가장 높은 확률).
이항분포(Binomial Distribution)가 다항분포(Multinomial Distribution)보다 더 자주
채택되는 이유는 주로 그 단순성과 적용 범위 때문

첨도와 왜도

표준화(Standardization)
•표준화는 데이터의 값을 평균이 0, 표준편차가 1이 되도록 변환.
•데이터가 정규분포에 가깝게 변환되며, 이상치의 영향을 상대적으로 덜 받는다.
•다양한 분포를 가지는 데이터를 동일한 기준으로 비교.
•선형 회귀, 로지스틱 회귀: 모델의 가중치가 큰 값을 갖지 않도록 하여 수렴을 빠르게
•PCA(주성분 분석): 데이터를 표준화하여 각 주성분이 동일한 중요도를 가지도록
scalser = StandardScaler()

min-max 정규화(Normalization)
•Min-Max 정규화는 데이터의 값을 특정 범위(보통 0과 1)로 변환
•데이터의 분포가 동일한 스케일로 조정되어 각 특성이 동일한 중요도를 가지게 된다.
•이상치(outliers)가 존재할 경우 정규화된 값들이 왜곡될 수 있다.
•Min-Max 정규화에서 최대값으로 나누지 않고 최소값을 고려하는 이유
데이터를 [0, 1] 범위로 정확하게 변환하고, 데이터의 상대적 위치와 간격을 유지하며,
다양한 특성 간의 스케일을 맞추기 위함
•이미지 처리: 픽셀 값의 범위가 0에서 255인 경우, 이를 0에서 1 사이로 정규화하여
모델의 학습을 용이하게.
•신경망 입력값: 각 입력값의 범위를 0과 1 사이로 정규화하여 학습 속도를 높이고,
수렴을 안정화.

확률 기초
확률(Probability)
설명: 어떤 사건이 일어날 가능성을 숫자로 나타낸 것.
범위: 0에서 1 사이의 값으로 표현,
 0은 사건이 절대 일어나지 않음/1은 사건이 반드시 일어남
예시: 동전을 던졌을 때 앞면이 나올 확률은 0.5.

분산(Variance)
설명: 확률 변수 값들이 기대값으로부터 얼마나 떨어져 있는지를 나타내는 지표.

Softmax 함수
머신러닝, 특히 딥러닝에서 다중 클래스 분류 문제를 해결하기 위해 사용
확률론에 기반하여, 주어진 입력 값을 확률 분포로 변환
로지스틱 회귀의 확장으로 볼 수 있으며, 각 클래스에 속할 확률을 계산
Softmax 함수의 정의
Softmax 함수는 주어진 입력 벡터 𝑧z의 각 요소를 [0, 1] 사이의 값으로 변환하며,
변환된 값들의 합은 1, 각 클래스가 선택될 확률.

데이터 전처리 

모델이 학습하기 좋은 정규 분포회 된 이상치 결측치를 없앤 이상적인 데이터셋

데이터 전치리(Data Preprocessing)
머신러닝 모델을 학습시키기 전에 데이터를 준비하고 정리하는 과정
데이터를 분석하고 모델의 성능을 최적화하기 위해 필수적
결측치(Missing Values)
: 데이터셋에 누락된 값이 있을 때 이를 처리하는 방법,
제거: 결측치가 있는 행 또는 열을 삭제.
대체: 결측치를 평균, 중간값, 최빈값 등으로 대체
데이터 정규화(Normalization)
: 데이터의 값을 일정한 범위로 변환
Min-Max 정규화를 사용하여 데이터를 [0, 1] 범위로 변환
표준화(Standardization)
: 데이터의 평균을 0, 표준편차를 1로 변환
Feature Engineering
모델의 성능을 향상시키기 위해 새로운 특성(Feature)을
만드는 과정

차원 축소(Dimensionality Reduction)
데이터의 특성 수를 줄여서 모델의 성능을 향상시키고 계산 효율성을 높이는 방법.
주성분 분석(PCA) 등을 사용하여 차원을 축소
라벨 인코딩(Label Encoding)
범주형 데이터를 숫자로 변환하는 과정.
원-핫 인코딩(one-hot encoding)
범주형 데이터를
해당 범주에만 1을, 나머지에는 0을 입력값 (0으로 채우고 해당 하는 곳만 1로 채움)
데이터 분할(train-test split)
모델의 학습과 평가를 위해 데이터를 훈련 세트와 테스트 세트로 나누는 과정(한번에 테스트 하지 않는다 이유는 잘 학습했는지 테스트 하기 위해사)

탐색적 데이터 분석(Exploratory Data Analysis)
데이터를 시각적으로 분석하고 기술 통계적으로 요약하여 주요 특징, 패턴, 이상치, 가설을 이해하는 과정

데이터 시각화(Data Visualization)
•설명: 데이터를 그래프로 시각적으로 표현하여 이해하는 과정.
•주요 용어:
• 히스토그램 (Histogram): 데이터의 분포를 보여주는 막대 그래프.
• 박스플롯 (Box Plot): 데이터의 분포와 이상치를 시각적으로 표현.
• 산점도 (Scatter Plot): 두 변수 간의 관계를 보여주는 점 그래프.
• 히트맵 (Heatmap): 변수 간의 상관 관계를 색상으로 표현.

이상치 탐지(outlier Detection)
•설명: 데이터 내에서 다른 값들과 크게 벗어난 값을 찾는 과정.
•주요 용어:
• 이상치 (Outliers): 데이터의 다른 값들과 비교하여 현저히 벗어난 값들.

탐색적 데이터 분석(Exploratory Data Analysis)
데이터를 시각적으로 분석하고 통계적으로 요약하여 주요 특징, 패턴, 이상치, 가설을
이해하는 과정
기술통계량 (Descriptive Statistics) : 데이터의 주요 특성을 요약하여 보여주는 통계량.
주요 측정 지표:
• 평균 (Mean): 데이터의 평균 값.
• 중앙값 (Median): 데이터의 중간 값.
• 분산 (Variance): 데이터 값들이 평균으로부터 얼마나 떨어져 있는지 나타내는 지표.
• 표준편차 (Standard Deviation): 분산의 제곱근, 데이터의 변동성을 나타냅니다.
• 최소값 (Min) / 최댓값 (Max): 데이터의 최소값과 최대값.
• 사분위수 (Quartiles): 데이터를 네 부분으로 나눈 값들.

상관 분석(Correlation Analysis)
•설명: 두 변수 간의 상관 관계를 분석하여 관계의 강도와 방향을 이해하는 과정.
•주요 용어:
• 상관계수 (Correlation Coefficient): 두 변수 간의 선형 관계의 강도와 방향을
나타내는 값 (-1에서 1 사이).
공분산(Covariance)
한 변수가 증가할 때 다른 변수가 어떻게 변하는지를 측정. 두 변수 간의 관게의 강도와 방향.
공분산이 양수인 경우 두 변수는
양의 관계를 가지고 있으며, 한 변수의 값이 증가하면 다른 변수의 값도 증가.
공분산이 0인 경우 두 변수는 서로 독립적이거나 선형적인 관계가 없다는 것을 의미.
공분산은 변수 선택, 차원 축소, 클러스터링, 생성 모델링 등 에서 활용.

공분산은 음수부터 양수까지의 모든 실수 값을 가질 수 있다.
범위는 무한대로 확장. 공분산 값의 범위는 −∞−∞에서 +∞+∞까지.
•양수 공분산: 두 변수가 함께 증가하는 경향이 있는 경우, 공분산은 양수 값.
•음수 공분산: 두 변수가 하나는 증가/다른 하나는 감소 경우, 공분산은 음수 값.
•공분산이 0: 두 변수 간에 선형 관계가 없거나 무상관인 경우, 공분산은 0.

VS. 상관계수는 공분산을 각 변수의 표준편차로 나눈 값으로,
-1에서 1 사이의 범위를 가지며,
 변수 간의 관계의 방향성과 강도를 더 정확하게 나타냅니다

 공분산: 방향성을 확인하고 변수의 원래 단위로 해석할 때, 스케일이 비슷한 경우 적합(예: 주식 수익률 간 방향성 분석)
 상관계수: 관계의 강도와 방향을 정량화하고, 스케일이 다르거나 비교가 필요한 경우 적합(예: 공부 시간과 점수 간 선형 관계 강도 분석)

 다중공선성(Multicolinearity)
 독립 변수들의 강한 선형 광계로 예측 모델의 정확도를 해치는 현상을 의미.
 몇 가지 부작용
첫째, 공선성으로 인해 회귀 계수의 추정값이 불안정
둘째, 공선성이 높은 독립 변수들은 다른 독립 변수들과 중복되는 정보를 제공
셋째, 공선성으로 인해 예측 모델의 일반화 성능이 저하

개념 주요 특징 차이점
공분산 (Covariance) 두 변수의 관계를 나타내는 값 양의 수 또는 음의 수로 나타남 금과 은의 가격변화는 같은 방향?
상관계수
(Correlation coefficient) 두 변수 사이의 선형 관계 강도와 방향을 나타냄 -1부터 1까지의 범위를 가짐 공부시간과 사험성적은 양의 선형관계
다중공선성
(Multicollinearity)
다중회귀 분석에서 독립 변수들 간의 상관 관계
회귀 모델의 성능에 부정적인 영향을 평가
회귀 모델에서 독립 변수들 간의
상관 관계를 검사하고 다룸
집값에 영향을 주는 중복요인

결정계수(Coefficient of Determination)
회귀 모델의 적합도를 평가하는 지표 R^2
결정 계수는 0부터 1까지의 값을 가지며, 1에 가까울수록 모델이 데이터를 더 잘 설명된다로 판단된다
회귀 모델이 데이터를 얼마나 잘 설명하는지를 측정

회귀 모델이 데이터를 얼마나 잘 설명하는지를 측정하는 것으로 해석될 수 있다.
SSR: 데이터를 설명하는 정도, 예측값과 종속 변수 평균값의 차이를 제곱/더한 값
SSE:설명을 못하는 오차(잔차), 예측값과 실제값의 차이를 제곱하여 더한 값
SST:전체 데이터의 변동성을 나타내는 값,전체 데이터의 값과 데이터의 평균값 사이의 차이(편차)를 제곱하여 모두 합한 값

선형대수
대수학(Algebra):'代數'는 '수를 대신하다’, 아랍어 'al-jabr’, 미지수를 '복원하는' 수학
숫자, 기호, 문자를 사용하여 방정식과 수식을 다루는 수학의 분야
•기본 대수학: 2𝑥+3=7 을 풀어 𝑥=2 를 찾기.
•추상 대수학: 주어진 집합과 연산으로 군(Group)을 정의하고 그 성질을 연구.

선형대수(Linear Algebra)
벡터, 벡터 공간, 행렬 등을 사용하여 선형 방정식을 다루는 수학의 분야
선형 대수는 벡터 공간과 선형 변환을 연구하는 수학의 한 분야
벡터와 행렬을 사용하여 복잡한 문제를 단순화하고 해결하는 데 사용.

벡터(Vector)
벡터는 크기와 방향을 가진 수학적 객체.

벡터화의 과정: 특성값들을 기준 축으로 삼아 좌표화 하는 과정
1.데이터(예: 이미지, 텍스트, 숫자 등)는 여러 개의 특성값(feature value)를 가지고 있다.
2.이런 특성값들 각각을 하나의 축(dimension)으로 간주.
3.각 특성값을 그 축 상의 좌표값으로 mapping.
4.이렇게 해서 만들어진 다차원 좌표들의 집합이 바로 벡터.
예를 들어 셀값은 RGB 3개의 특성값을 가지므로 3차원 공간에 mapping
각 픽셀의 R, G, B값이 (x, y, z) 좌표가 되어 최종적으로 벡터 v = (r, g, b)가 만들어지는 것

벡터터화 및 좌표화가 필요한 주요 이유
1. 수치 연산 효율성 증대
벡터/행렬 연산이 반복적인 스칼라 연산보다 계산 효율적
특히 대규모 데이터에 대한 병렬 처리 시 성능 향상
2. 기하학적 해석 가능성
벡터는 크기와 방향을 가지므로 기하학적 의미 부여 가능데이터 간 유사도,
군집화 등을 시각적으로 해석 용이
3. 기계학습 모델의 입력 형태
대부분의 기계학습 알고리즘은 벡터/텐서 형태의 입력 데이터
요구데이터를 벡터 공간에 Mapping해야 모델 학습 가능
4. 차원 축소 및 시각화
고차원 데이터를 저차원 벡터 공간에 투영하여 해석 용이
차원 축소를 통해 데이터 시각화 및 패턴 파악 가능

내적(Dot product)
1. 단순 벡터 내적
•공식: A · B = Σ(A_i * B_i)
•두 벡터의 성분별 곱의 합
•벡터의 크기와 방향 모두 고려
•값의 범위: 0 이상의 모든 실수값
•용도: 기하학적 프로젝션, 신호 프로세싱, 물리학 등
2, 코사인 유사도
•공식: cos(θ) = (A · B) / (||A|| * ||B||)
•두 벡터 사이의 코사인 각도를 이용하여 유사도 계산
•벡터의 절대적인 크기(magnitude)는 무시하고 방향(direction)만 고려
•값의 범위: [-1, 1]
•용도: 텍스트 유사도, 추천 시스템 등에서 벡터 유사도 측정
•벡터의 내적에 코사인(cosine) 값을 곱하는 이유는 벡터 간의 유사도를 0에서 1 사이의
범위로 정규화(normalization)하기 위함. 해석하기 쉽다

내적: 두 벡터의 내적은 대응하는 성분끼리 곱한 후, 그 곱을 모두 더한 값

벡터의 정사영(Projection)
A 벡터의 성분 중 B 벡터와 평행한 성분만을 추출하는 작업
B 벡터 방향으로 A를 정사영한 벡터의 크기는 A 벡터가 B 벡터 방향으로 얼마나 큰 '성분'을
가지고 있는지를 나타낸다.
정사영을 통해 두 벡터가 같은 방향을 가리키게 했을 때, 그 크기를 비교할 수 있다.

두 벡터를 더할 때, 대응하는 성분끼리 더합니다.
벡터의 각 성분을 스칼라(실수)로 곱합니다.
두 벡터의 내적은 대응하는 성분끼리 곱한 후, 그 곱을 모두 더한 값입니다.
두 행렬을 더할 때, 대응하는 성분끼리 더합니다.
두 행렬의 곱은 첫 번째 행렬의 행과 두 번째 행렬의 열의 내적을 계산하여 새로운
행렬을 만듭니다

전치행렬,행렬 분해, 고유값 분해

머신러닝에서 선형대수가 사용되는 예시

1. 이미지 처리와 컨볼루션 연산
이미지는 픽셀 값들의 행렬로 표현.
이미지에서 특정 패턴(엣지, 모서리 등)을 찾기 위해서는 입력 이미지 행렬에 필터
행렬(가중치 행렬)을 적용하는 컨볼루션 연산을 수행합니다. 이때 선형대수의 행렬
곱셈이 사용.

2. 단어 벡터 표현과 유사도 게산
텍스트 데이터에서 각 단어를 벡터로 표현.
예를 들어 "안녕"은 (1, 0, 0), "잘가" (0, 1, 0), "시간" (0, 0, 1) 등으로 벡터화. 이후 문장은
단어 벡터들의 합으로 표현.
두 문장 벡터 사이의 유사도를 구하기 위해서는 벡터 내적을 계산. 내적 값이 클수록 두
문장이 비슷한 의미를 가지고 있다고 판단. 

트랜스포머는 임베딩 벡터 내적값 유사도를 구한 행렬값

미적분
미분(Differentiation)
미분은 함수의 변화율을 계산하는 과정,
함수가 어떻게 변하는지를 나타내며, 기울기를 통해 표현.
기울기 (Slope): 함수의 그래프에서 한 점에서의 기울기는 그 점에서의 순간 변화율

도함수 x에서 함수의 변화율을 나타냅니다.,접선 곡선의 한 점에서 그 점에서의 기울기와 동일한 직선,연쇄 법칙 합성 함수의 미분법, g(f(x))의 도함수를 구할 때 사용.

Constant Rule (상수 규칙) * 중요
Power Rule (거듭제곱 규칙) * 중요
Product Rule (곱셈 규칙)
Quotient Rule (나눗셈 규칙)
Chain Rule (연쇄 규칙) * 중요 역전파

연쇄 법칙: 딥러닝에서 오차함수의 최적화를 위해 역전파를 시행할 때 연쇄법칙에 의한 미분값(가중치)를 계산하게 된다

경사하강법(Gradient Descent) * 중요 손실값,예측값의 가중치의 기울기 값으로 오차를 줄여간다
머신러닝,딥러닝의 목표는 오차를 좁히는것 모델의 결과값이 실제값과의 오차를 줄이는것 이 방법중 하나가 경사하강법의 미분

적분(Intergration)
함수의 넓이나 누적 합을 계산하는 과정
함수의 값을 일정 구간에 대해 더한 결과

넓이(Area) 적분은 함수의 그래프 아래의 넓이를 계산하는데 사용됩니다.

최적화(Optimization)
최적화는 함수의 최대값이나 최소값을 찾는 과정
함수의 극대점이나 극소점을 찾는 것

극대점 (Maximum Point)
극소점 (Minimum Point)
임계점 (Critical Point)

2차 도함수 (Second Derivative)
(함수의 1차 도함수를 다시 미분한 것으로, 함수의 곡률을 나타냄)
2차 도함수를 통해 임계점이 극대점인지 극소점인지 판단할 수 있다.

교차 검증(Cross-Validation)
모델의 성능을 보다 안정적이고 일반화된 방식으로 평가하기 위해 데이터를 여러
부분으로 나누어 여러 번 학습하고 평가하는 기법.

k-겹 교차 검증(K-Fold Cross-Validation)
데이터를 K개의 부분으로 나누고, 각 부분을 한 번씩 테스트 세트로 사용하며 나머지
K-1 부분을 훈련 세트로 사용하여 모델을 학습하고 평가하는 방법

1.데이터를 K개의 부분으로 분할.
2.각 부분을 한 번씩 테스트 세트로 사용하고, 나머지를 훈련 세트로 모델을 학습.
3.K번의 학습과 평가를 통해 모델의 평균 성능을 계산.

# K-fold Cross Validation 수행
scores = cross_val_score(model, iris.data, iris.target, cv=5)

오버피팅(Overfitting)
모델이 학습 데이터에 너무 잘 맞추어져서 새로운 데이터(검증 데이터나 테스트
데이터)에 대해 일반화되지 않는 문제
모델이 학습 데이터의 잡음이나 세부사항까지 과도하게 학습했기 때문에 발생
특징
•학습 데이터에 대한 성능만은 매우 좋지만, 새로운 데이터에 대한 성능은 떨어집니다.
•모델이 복잡하고 파라미터 수가 많을 때 발생하기 쉽습니다.

언더피팅(Underfitting)
모델이 학습 데이터의 패턴을 충분히 학습하지 못하여 학습 데이터와 새로운 데이터
모두에 대해 성능이 낮은 문제
모델이 너무 단순하여 데이터의 중요한 특징을 잡아내지 못하기 때문에 발생

모델의 복잡도
너무 낮으면 단순화의 문제(언더핏팅)가,
너무 높으면 과잉 적합의 문제(오버핏팅)가 발생
두 극단 사이의 적절한 복잡도에서 일반화 성능이 가장 좋은 베스트 핏 지점
따라서 모델 선택 시 복잡도를 고려하여 훈련/테스트 오차의 균형을 맞춰야
좋은 일반화 성능을 얻을 수 있다.

모델의 최적화
1. 손실 함수(Loss Function)
모델의 예측 값과 실제 값 간의 차이를 측정하는 함수
모델이 얼마나 잘못 예측했는지를 나타내며, 손실 함수의 값을 최소화하는 것이 목표
주요 손실 함수
평균 제곱 오차(Mean Squared Error,Mse)예측 값과 실제 값의 차이를 제곱한 후
평균을 구한 것.
교차 엔트로피 손실(Cross-Entropy Loss)s분류 문제에서 사용되는 손실 함수로, 예측
확률과 실제 레이블 간의 차이를 측정합니다.

2. 경사 하강법(Gradient Descen)
손실 함수를 최소화하기 위해 모델의 가중치를 반복적으로 업데이트하는 최적화
알고리즘. 손실 함수의 기울기(경사)를 따라 내려가면서 최적의 가중치를 찾는다.
•배치 경사 하강법 (Batch Gradient Descent): 전체 데이터셋을 사용하여 기울기를 계산.
데이터셋이 클 경우 메모리 사용량과 계산 비용이 급증
•확률적 경사 하강법 (Stochastic Gradient Descent, SGD):
하나의 데이터 포인트를 사용하여 기울기를 계산.
•미니배치 경사 하강법 (Mini-Batch Gradient Descent):
작은 데이터셋(배치)을 사용하여 기울기를 계산.

3. 학습률(Learning Rate)
경사 하강법에서 가중치를 업데이트할 때 사용하는 스텝 크기. 학습률은 너무
크거나 작으면 최적화 과정에 문제를 일으킬 수 있다.
•높은 학습률: 빠르게 수렴하지만 최적의 값을 놓칠 수 있다.
•낮은 학습률: 천천히 수렴하지만 더 정확한 최적의 값을 찾을 수 있다.
•대부분의 0.001이 딥러닝 모델 학습의 표준 초기값으로 널리 사용

4. 정규화(Regularization)
모델의 복잡성을 제어하고 오버피팅을 방지하기 위해
손실 함수에 패널티를 추가하는 방법.
(일부 변수의 영향을 감소시켜 모델을 단순하게 바꿔보는 시도)
• L1 정규화 (Lasso Regularization): 가중치의 절대값 합을 손실 함수에 추가
Loss Function=Original Loss+𝜆∑∣𝑤𝑖∣Loss Function=Original Loss+λ∑∣wi∣
• L2 정규화 (Ridge Regularization): 가중치의 제곱 합 을 손실 함수에 추가
Loss Function=Original Loss+𝜆∑𝑤𝑖2Loss Function=Original Loss+λ∑wi2

5. 조기 종료(Early Stopping)
검증 데이터의 성능이 더 이상 향상되지 않을 때 학습을 멈추는 기법.
오버피팅을 방지하는 데 도움.

6.하이퍼파라미터 튜닝(Hyperparameter Tuning)
모델의 하이퍼파라미터(학습률, 배치 크기, 정규화 계수 등)를 최적화하는 과정.
•그리드 서치 (Grid Search): 가능한 모든 하이퍼파라미터 조합을 시도
•랜덤 서치 (Random Search): 무작위로 선택된 하이퍼파라미터 조합을 시도

모델의 적용(deployment)
Training->Validation->Deployment->Monitoring

이진분류 모델 주요용어
confusion matrix  IEEE ACCESS paper 이걸 참조
TP FP 옳은예측(긍정) 틀린예측(긍정) -> Precision
FN TN 틀린예측(부정) 틀린예측(부정)
        Recall                       Accuracy
        