랜덤 포레스트(random forest)
앙상블 학습 방법의 일종으로,
데이터학습 과정에서 구성한 다수의 결정 트리로부터 분류 또는 평균 예측치를
출력함으로써 동작.
일부 의사결정트리들이 오버피팅을 보일 수는 있지만 다수의 의사결정트리
결과값을 기반으로 예측(voting)하기 때문에 좋은 결과값을 보인다.

배깅(Bagging : bootstrap + aggregating)
데이터세트에서 중복을 허용해서 샘플자료를 뽑아(bootstrap)
여러 개의 의사결정트리를 구성하여 결과를 취합한다(aggregating)
예시) 학습 데이터 세트에 총 1000개를 가정시 임의로 중복하여 100개 선택해서
의사결정트리를 만들어 그 결과값을 취합하는 것이 배깅(bagging).
학습 데이터의 일부를 중복되게 랜덤으로 의사결정 트리 생성 !

부트스트랩(Bootstrap)이란,
일반적으로 한 번 시작되면 알아서 진행되는 일련의 과정을 뜻한다.
원래의 부트스트랩은 Boot + strap으로, 긴 부츠의 뒷부분에 달린 고리를 뜻했다.
통계학 : 중복되게 샘플을 추출하는 기법.(복원추출)

* 부트스트랩(Bootstrap),
중복되게 샘플을 추출하는 이유.
1.통계적 추론: 중복 추출을 통해 생성된 재표본을 사용하여 모집단에 대한 통계적인
추론을 수행할 수 있다.
2.모델 평가: 분류 모델의 경우 중복 추출을 통해 재표본을 생성하고, 이를 사용하여
모델의 예측 성능을 평가, 모델의 일반화 성능을 추정 가능
3.파라미터 추정: 중복 추출을 통해 생성된 재표본을 사용하여 모델의 파라미터를
추정할 수 있다. 이를 통해 모델의 파라미터에 대한 불확실성을 평가할 수 있다.
=> 동일한 모집단에서 추출하므로 모집단의 특성을 정확하게 반영할 수 있다.
=> 특정 상황에서는 다른 추출 방법이 더 적합할 수도 있다.
분석 목적과 데이터 특성에 따라 결정

Random selection of Features
의사결정 트리의 본래 모델은 트리를 만들 때 모든 속성들을 살펴보고 정보
획득량이 가장 많은 속성(중요한 질문)을 선택해서 데이터를 분할한다.
전체 속성들 중 일부만 랜덤하게 고려하여 트리를 작성하여 다수의 트리를
만들고 그 결과값이 나오면 voting/aggregating을 통해 최종 결과값을 결정

Random selection of Features
예시) 30개의 속성(features)가 있다면 3개씩 임의로 features(질문)을 뽑아서
의사결정 트리를 구성하고 결과값을 추출해 낸다

Random selection of Features
과적합 방지: 랜덤 포레스트는 부트스트랩 샘플링과 무작위 특징 선택을 통해
다양성을 유지하고, 결정 트리들의 상호 작용을 제한하여 과적합을 줄인다.
높은 예측 성능: 여러 개의 결정 트리를 조합하여 예측을 수행하기 때문에, 개별
트리보다 더욱 강력한 예측 성능을 제공.

⚫ Random selection of Features
과적합 방지: 랜덤 포레스트는 부트스트랩 샘플링과 무작위 특징 선택을 통해
다양성을 유지하고, 결정 트리들의 상호 작용을 제한하여 과적합을 줄인다.

⚫ 앙상블 기법의 부스팅(Boosting)
- 분류기의 과거 학습 결과를 토대로 학습결과가 좋지 않은(Weak learner)분류기의
학습 데이터 샘플에 가중치를 부여하여 좀더 정확도 높은 모델을 만들어가는 방법.
- 잘못 분류된 데이터 포인트의 특성을 이해하고 이를 바탕으로 학습을 개선함으로써,
모델은 유사한 특성을 갖는 새로운 데이터에 대해서도 더 정확하게 예측할 수 있게
됩니다

⚫ 앙상블 기법의 부스팅(Boosting)
- 오류 최소화: 부스팅은 주로 오류를 최소화하는 데 집중합니다. 잘못 분류된
데이터에 높은 가중치를 부여함으로써, 다음 학습기는 이 데이터를 올바르게
분류할 가능성을 높이도록 학습합니다.
- 성능 향상: 가중치 조정을 통해, 각 학습기는 이전 학습기가 어려워한 데이터
포인트에 더 많은 리소스를 할당하게 되므로, 전체적인 모델의 오류율이 점차
줄어듭니다.

 Ensemble in machine learning
여러 개의 예측 모델을 결합하여 더 강력하고 정확한 예측을 수행하는 기법을 의미.
앙상블은 단일 모델보다 더 좋은 예측 성능을 제공하고,
모델의 안정성을 향상시키는 데에 활용

보팅(Voting):
하드 보팅(Hard Voting): 다수의 모델이 예측한 결과를 다수결로 결정하는 방식.
분류 문제에서 다수의 분류 모델의 예측 결과를 투표하여 최종 예측을 결정.
소프트 보팅(Soft Voting): 다수의 모델이 예측한 클래스 확률을 평균하여 예측.
각 모델의 예측 확률을 평균하여 가장 높은 확률을 가진 클래스를 최종 예측으로 선택.

배깅(Bagging):
랜덤 포레스트(Random Forest): 다수의 의사결정나무(Decision Tree)를 결합하여
예측을 수행하는 앙상블 모델. 각 의사결정나무는 부트스트랩 샘플링으로 구성된
데이터를 사용하여 학습하고, 결과를 결합하여 최종 예측을 수행.

Ensemble in machine learning
부스팅(Boosting): 이전 학습기 weak learner의 강화가 목적
AdaBoost(Adaptive Boosting): 이전 학습기가 잘못 분류한(Weak Learner)의
샘플들에 대해 집중하여 학습시켜 강한 학습기(Strong Learner)를 구성하는 앙상블
모델. 이전 모델의 오차를 보완하도록 샘플의 가중치를 조정하며 학습을 진행.
Gradient Boosting: 이전 학습기 모델의 오차인 잔차(residual)를 최소화 하기 위해
새로운 모델을 학습시키는 방식으로 오차를 보완하는 앙상블 모델.
(잔차는 실제 관측값과 모델에 의해 예측된 값 사이의 차이)

부스팅(Boosting):
Gradient Boosting: 이전 모델의 예측과 실제값의 차이인 잔차오류(residual
error)를 최소화하기 위해 새로운 모델을 학습시키는 방식으로 오차를 보완하는
앙상블 모델.

스태킹(Stacking):
다양한 기본 모델들을 학습시킨 후, 그 예측 결과를 기반으로 메타 모델을 학습시키는
방식.
기본 모델들의 예측 결과를 입력으로 사용하여 메타 모델을 학습하고 최종 예측을 수행.
메타 모델은 다양한 머신러닝 알고리즘을 사용할 수 있으며, 일반적으로는 선형
모델이나 신경망 모델 등을 사용

Stacked Ensemble Model
여러 개의 기본 모델들과 메타 모델을 결합하여 구성된 앙상블 모델.
기본 모델들은 서로 다른 알고리즘이나 파라미터 설정을 가진 다양한
모델들로 구성되며, 각각의 기본 모델은 독립적으로 학습하고 예측을 수행.
이후, 기본 모델들의 예측 결과를 메타 모델의 입력으로 사용하여 최종 예측.

⚫ Stacked Ensemble Model
스태킹에서 메타모델은
여러 기본 모델(Base Models)의 예측 결과를 입력으로 받아, 이들을 결합하여
최종 예측을 수행하는 역할을 합니다.
기본 모델들이 1차 예측을 수행하고, 메타모델이 이 예측 결과를 다시
학습하여 최종 예측을 하므로, 메타모델은 "예측을 예측하는 모델"이라고 할
수 있습니다.
기본 모델들이 서로 다른 오류를 가지기 때문에, 메타모델은 이들의 예측을
조합함으로써 개별 모델의 약점을 보완하고, 전체적인 예측 성능을 개선할 수
있습니다.

① 지도학습
- Extra Trees(Extremely Randomized Trees), 앙상블의 한 종류
계산 비용이 낮아 일반적으로 더 빠른 학습 및 예측 속도를 제공.
결정 트리와 Extra Trees의 주요 차이점은 랜덤성의 정도.
Extra Trees는 분할에 대한 특성의 무작위 선택을 강화하며, 이는 모델의 다양성을
증가시키고 과적합을 줄이는 효과.
결정 트리는 특성값의 최적 선택을 위해 불순도 지표를 사용하여 분할을 수행.
데이터셋에 따라 Extra Trees가 결정 트리보다 더 나은 성능을 보일 수 있으며,
특히 노이즈가 많거나 복잡한 데이터에 적합.
모델의 해석력이나 설명력이 중요한 경우에는 결정 트리를 선호할 수 있다.

- Extra Trees(Extremely Randomized Trees)
주요 특징과 차이점
1.랜덤한 특성 선택: 결정 트리와 달리 특성의 임계값을 찾는 데 더 많은 무작위성.
2.랜덤한 임계값 선택: Extra Trees는 분할을 할 때, 특성의 임계값을 선정하는
과정에서 랜덤한 임계값을 선택. 더 많은 다양성.
3.부트스트래핑: 샘플링에서 무작위성을 추가로 도입하여 다양한 모델을 생성.
4.예측 방법: 평균 또는 다수결 방식으로 결합하여 최종 예측을 수행.

Extra Trees의 주요 차별점:
특성 선택의 무작위성:
기존 방식: 각 노드에서 모든 특성을 고려하여 최적의 분할 지점을 찾습니다.
Extra Trees: 각 노드에서 임의의 특성 부분집합만을 고려합니다.
분할 임계값의 무작위성:
기존 방식: 선택된 특성에 대해 최적의 분할 임계값을 찾습니다.
Extra Trees: 각 특성에 대해 임의의 분할 임계값을 생성합니다.
앙상블 구성:
기존 랜덤 포레스트: 부트스트랩 샘플링으로 다양한 훈련 세트를 만듭니다.
Extra Trees: 전체 훈련 세트를 사용하여 각 트리를 학습시킵니다

파라미터
randomforestclassifier
n_estimators: 앙상블에 사용할 의사 결정 트리의 개수를 지정. 기본값은 100.
criterion: 분할 기준을 지정. "gini" 또는 "entropy"를 선택. 기본값은 "gini".
max_depth: 의사 결정 트리의 최대 깊이를 제한. 정수 값을 입력하거나 None을
사용하여 깊이를 제한하지 않을 수 있다. 기본값은 None.
min_samples_split: 분할을 위해 노드에 필요한 최소 샘플 수를 지정. 정수 값을
입력하거나 실수 값을 사용하여 전체 샘플의 비율을 지정할 수 있다. 기본값은 2
min_samples_leaf: 리프 노드에 필요한 최소 샘플 수를 지정. 정수 값을
입력하거나 실수 값을 사용하여 전체 샘플의 비율을 지정할 수 있다. 기본값은 1.
max_features: 각 의사 결정 트리에서 고려할 특성의 개수 또는 비율을 지정.
정수 값을 입력하거나 실수 값을 사용하여 전체 특성 중 일부를 사용할 수 있다.
기본값은 "auto"로 sqrt(n_features)를 의미.
random_state: 재현 가능한 결과를 얻기 위한 난수 시드(랜덤 시드)를 지정.

randomforestregressor
n_estimators: 앙상블에 사용할 의사 결정 트리의 개수. 기본값은 100.
criterion: 분할 기준을 지정. "mse" (평균 제곱 오차) 또는 "mae" (평균 절대 오차)를
선택할 수 있다. 기본값은 "mse".
max_depth: 의사 결정 트리의 최대 깊이를 제한. 정수 값을 입력하거나 None을
사용하여 깊이를 제한하지 않을 수 있다. 기본값은 None.
min_samples_split: 분할을 위해 노드에 필요한 최소 샘플 수를 지정. 정수 값을
입력하거나 실수 값을 사용하여 전체 샘플의 비율을 지정할 수 있다. 기본값은 2.
min_samples_leaf: 리프 노드에 필요한 최소 샘플 수를 지정. 정수 값을 입력하거나
실수 값을 사용하여 전체 샘플의 비율을 지정할 수 있다. 기본값은 1
max_features: 각 의사 결정 트리에서 고려할 특성의 개수 또는 비율을 지정. 정수
값을 입력하거나 실수 값을 사용하여 전체 특성 중 일부를 사용할 수 있다. 기본값은
"auto"로 sqrt(n_features).
random_state: 재현 가능한 결과를 얻기 위한 난수 시드(랜덤 시드)를 지정

bagginclassifier
base_estimator: 기본 분류기 모델을 지정. 일반적으로 의사 결정
트리(DecisionTreeClassifier)가 사용되지만, 다른 분류기도 사용할 수 있다. 기본값은 DT
n_estimators: 앙상블에 사용할 기본 분류기의 개수. 많은 수의 기본 분류기를
사용할수록 성능이 향상될 수 있지만, 계산 비용이 증가. 기본값은 10.
max_samples: 각 기본 분류기에 사용할 훈련 샘플의 개수 또는 비율. 정수 값을
입력하면 해당 개수의 샘플을 사용하고, 실수 값을 입력하면 전체 샘플 중 일부를 사용.
기본값은 1.0 (전체 샘플 사용).
max_features: 각 기본 분류기에 사용할 특성의 개수 또는 비율을 지정. 정수 값을
입력하면 해당 개수의 특성을 사용하고, 실수 값을 입력하면 전체 특성 중 일부를 사용.
기본값은 1.0 (전체 특성 사용).
bootstrap: 재샘플링 시에 중복을 허용할지 여부를 지정. True로 설정하면 중복을
허용하고, False로 설정하면 중복을 허용하지 않는다. 기본값은 True.
bootstrap_features: 재샘플링 시에 특성도 중복을 허용할지 여부를 지정. True로
설정하면 중복을 허용하고, False로 설정하면 중복을 허용하지 않는다. 기본값은
False.
random_state: 재현 가능한 결과를 얻기 위한 난수 시드(랜덤 시드)를 지정. 다른
난수 시드를 사용하면 다른 결과를 얻게 된다.

bagginregressor
base_estimator: 기본 회귀 모델을 지정. 일반적으로 의사 결정
트리(DecisionTreeRegressor)가 사용되지만, 다른 회귀 모델도 사용할 수 있다.
기본값은 의사 결정 트리.
n_estimators: 앙상블에 사용할 기본 회귀 모델의 개수를 지정. 일반적으로 많은
수의 기본 모델을 사용할수록 성능이 향상될 수 있지만, 계산 비용이 증가.
기본값은 10.
max_samples: 각 기본 회귀 모델에 사용할 훈련 샘플의 개수 또는 비율을 지정.
정수 값을 입력하면 해당 개수의 샘플을 사용하고, 실수 값을 입력하면 전체 샘플
중 일부를 사용. 기본값은 1.0 (전체 샘플 사용).
max_features: 각 기본 회귀 모델에 사용할 특성의 개수 또는 비율을 지정. 정수
값을 입력하면 해당 개수의 특성을 사용하고, 실수 값을 입력하면 전체 특성 중
일부를 사용. 기본값은 1.0 (전체 특성 사용).
bootstrap: 재샘플링 시에 중복을 허용할지 여부를 지정. True로 설정하면 중복을
허용하고, False로 설정하면 중복을 허용하지 않는다. 기본값은 True.
random_state: 재현 가능한 결과를 얻기 위한 난수 시드(랜덤 시드)를 지정

extratreesclassifier
n_estimators: 앙상블에 사용할 Extra Trees의 개수를 지정. 일반적으로 많은 수의
트리를 사용할수록 성능이 향상될 수 있지만, 계산 비용이 증가. 기본값은 100
criterion: 분할 기준을 지정합니다. "gini" 또는 "entropy"를 선택할 수 있다.
기본값은 "gini".
max_depth: 의사 결정 트리의 최대 깊이를 제한. 정수 값을 입력하거나 None을
사용하여 깊이를 제한하지 않을 수 있다. 기본값은 None.
min_samples_split: 분할을 위해 노드에 필요한 최소 샘플 수를 지정. 정수 값을
입력하거나 실수 값을 사용하여 전체 샘플의 비율을 지정할 수 있다. 기본값은 2.
min_samples_leaf: 리프 노드에 필요한 최소 샘플 수를 지정. 정수 값을 입력하거나
실수 값을 사용하여 전체 샘플의 비율을 지정할 수 있다. 기본값은 1.
max_features: 각 의사 결정 트리에서 고려할 특성의 개수 또는 비율을 지정. 정수 값을
입력하거나 실수 값을 사용하여 전체 특성 중 일부를 사용할 수 있다. 기본값은 "auto"로
sqrt(n_features)를 의미.
bootstrap: 재샘플링 시에 중복을 허용할지 여부를 지정. True로 설정하면 중복을
허용하고, False로 설정하면 중복을 허용하지 않는다. 기본값은 False.
random_state: 재현 가능한 결과를 얻기 위한 난수 시드(랜덤 시드)를 지정. 다른 난수
시드를 사용하면 다른 결과를 얻게 된다.

extratreesregressor
n_estimators: 앙상블에 사용할 Extra Trees의 개수를 지정. 기본값은 100
criterion: "mse" (평균 제곱 오차) 또는 "mae" (평균 절대 오차)를 선택할 수 있다.
기본값은 "mse".
max_depth: 의사 결정 트리의 최대 깊이를 제한. 정수 값을 입력하거나 None을
사용하여 깊이를 제한하지 않을 수 있다. 기본값은 None.
min_samples_split: 분할을 위해 노드에 필요한 최소 샘플 수를 지정. 정수 값을
입력하거나 실수 값을 사용하여 전체 샘플의 비율을 지정할 수 있다. 기본값은 2.
min_samples_leaf: 리프 노드에 필요한 최소 샘플 수를 지정. 정수 값을 입력하거나
실수 값을 사용하여 전체 샘플의 비율을 지정할 수 있다. 기본값은 1.
max_features: 각 의사 결정 트리에서 고려할 특성의 개수 또는 비율을 지정. 정수 값을
입력하거나 실수 값을 사용하여 전체 특성 중 일부를 사용할 수 있다. 기본값은 "auto"로
sqrt(n_features)를 의미.
bootstrap: 재샘플링 시에 중복을 허용할지 여부를 지정. True로 설정하면 중복을
허용하고, False로 설정하면 중복을 허용하지 않는다. 기본값은 False.
random_state: 재현 가능한 결과를 얻기 위한 난수 시드(랜덤 시드)를 지정