현재 위치에서 행동을 해야만 다음 상태와 보상을 알 수 있다.
그러나 목표(Goal)에 도달하기 까지 Hole이 존재하여 실패할 가능성도 존재한다.
항상 최대의 Reward를 추구한다는 가정(argmax(Q[state]) )!!!
Q(s, a) = r + max Q(s’, a’)
Dummy Q-learning algorithm
1. Q 테이블 초기화
모든 상태 s와 행동 𝑎에 대해 Q값 𝑄^(𝑠,𝑎)를 0으로 설정합니다.
2. 시작 상태 관찰
현재 상태 s를 확인합니다.(예: 에이전트가 시작 지점에 있음)
3. 무한 루프 수행 (학습 반복)
a. 행동 선택 & 실행
같은 Q값이면 랜덤하게 행동
어떤 행동 𝑎를 골라서 실행합니다.(예: "오른쪽으로 이동")
b. 행동을 하면 보상 수신 즉시 보상 𝑟을 받습니다.(예: 도착했다면 보상 1, 실패면 0)
c. 다음 상태/행동을 위해 새 상태 𝑠′ 를 설정합니다.
d. Q 테이블 갱신
보상과 다음 상태의 최적 Q값을 통해 현재 행동의 가치(Q)를 업데이트

강화학습 (Reinforcement Learning)
강화학습은 에이전트가 환경과 상호작용하면서, 행동의 결과로 받는 보상에 따라 
어떤 전략(정책)이 가장 좋은지를 스스로 학습하는 방법론이다. 
에이전트는 시간에 따라 상태를 관찰하고, 
그에 맞는 행동을 선택하며, 환경은 이에 대한 보상과 다음 상태를 반환한다. 
이 과정을 반복하면서 에이전트는 보상을 최대화하는 방향으로 정책을 개선해 나간다.

강화학습 문제는 일반적으로 마르코프 결정 과정(MDP)이라는 수학적 모델 위에 정의된다.
 MDP에서는 상태, 행동, 상태 전이 확률, 보상 함수, 
그리고 미래 보상의 중요도를 조절하는 할인율로 구성된다. 
이 모델은 현재 상태만으로 미래를 예측할 수 있다는 마르코프 성질을 갖는다.

에이전트는 특정 정책을 따르며 행동을 선택하는데, 
이 정책은 상태에 따라 어떤 행동을 선택할 확률을 정의한다. 
정책의 품질은 "가치 함수"라는 개념으로 측정되며
 가치 함수는 특정 상태나 상태-행동 쌍에서 장기적으로 기대되는 누적 보상의 크기를 나타낸다.

강화학습의 목적은 보상의 기대값이 가장 높은 최적 정책을 찾는 것이다. 
이를 위해 다양한 알고리즘이 제안되며, 가치 기반 방법, 정책 기반 방법, 
또는 이 둘을 결합한 액터-크리틱 방식이 있다. Q-learning

Q-learning은 강화학습의 대표적인 가치 기반 방법이다. 이 방법은 상태와 행동의 쌍에 대해, 
해당 행동을 했을 때 향후 받을 수 있는 누적 보상을 추정하는 값을 저장하고 학습한다. 
이 값을 Q값이라고 하며, 에이전트는 
Q값이 가장 높은 행동을 선택하는 방식으로 정책을 따르게 된다.
Q-learning은 모델 프리(model-free) 알고리즘이며, 
환경의 전이 확률이나 보상 구조를 명시적으로 알 필요 없이 학습이 가능하다. 
이 알고리즘은 현재 Q값과 새로운 경험을 바탕으로 Q 테이블을 점진적으로 보정하며 
최적 Q값에 수렴하도록 한다.
 학습 과정에서는 현재 상태에서 어떤 행동을 선택할지를 결정하기 위해, 
보상이 가장 클 것으로 예측되는 행동을 선택하거나, 
때때로 무작위 행동을 선택하여 탐험과 활용 사이의 균형을 조절한다.

Q-learning은 이론적으로 모든 상태-행동 쌍이 충분히 자주 탐색되고,
 학습률이 점차 감소할 경우 최적 Q값에 수렴한다는 것이 증명되어 있다.
 따라서 작은 상태 공간에서는 효과적으로 최적 정책을 학습할 수 있으며, 
상태 공간이 클 경우에는 딥러닝을 통해 Q함수를 근사하는 DQN(Deep Q-Network)으로 확장된다.

요약하면, 
강화학습은 보상 기반의 의사결정 학습 틀이며, 
Q-learning은 이 틀 안에서 동작하는 구체적인 알고리즘으로, 
가치 함수를 직접 추정하고 이를 통해 최적 정책을 유도한다.

action = np.argmax(Q[state, :] + np.random.randn(1, env.action_space.n) / (i + 1))
  목적은?  
"Q값을 기준으로 행동을 고르되, 가끔은 실수도 하면서 다양하게 행동해 보자!"
1.Q[state, :]
→ 현재 상태에서 가능한 4가지 행동의 Q값 (좌, 하, 우, 상)이 들어 있습니다.

예를 들어: [0.1, 0.4, 0.2, 0.0]
2. np.random.randn(1, 4)

→ 길이가 4인 랜덤 숫자들 (정규분포에서 뽑음).

예: [0.3, -1.1, 0.6, -0.2]
3. / (i + 1)

→ 이 랜덤 숫자들은 점점 작게 줄어듬 .
학습 초반 (i가 작을 때): 랜덤값 영향이 큼
학습 후반 (i가 커질 때): 랜덤값이 거의 0 → 실수 안 하고 정확히 선택
  4. 전체 더하기:  
  예시: 
Q[state, :] = [0.1, 0.4, 0.2, 0.0]노이즈 =       [0.3, -1.1, 0.6, -0.2] / (i+1)
합쳐서 =       [0.4, -0.7, 0.8, -0.2]
5. np.argmax(...)
→ 가장 큰 값을 가진 행동의 인덱스(방향) 선택!
예: 0.8 → 오른쪽(우)
 이 방법은 초기에는 노이즈가 크고, 점점 학습이 진행될수록 노이즈를 줄여 :
처음엔 다양한 행동을 시도하도록 유도 (탐험 exploration)
후반엔 학습된 Q값을 더 믿고 결정 (활용 exploitaion)
i가 작을 때 → 1 / (i+1)이 크기 때문에 노이즈 영향이 큼→ 다양한 행동을 선택할 가능성 ↑
i가 클 때 → 노이즈가 작아져서 거의 Q 값이 큰 쪽 선택

   Q-learning에서 중 하나로, 또는 에 가까운 방법입니다.

