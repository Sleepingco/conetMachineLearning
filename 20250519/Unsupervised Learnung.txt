비지도학습(Unsupervised Learning)
정답데이터가 없는 상태에서 입력데이터만 이용해서 컴퓨터를 학습시키는 방법.
예시) 뉴스 기사 분류, DNA 분류, SNS 관계 분류 등
비지도 학습은 예측이 목적이 아니라,
데이터의 구성 또는 특징을 밝히는 목적으로 사용되는 그룹핑 알고리즘
대표 알고리즘 : 군집화(Clustering) ,
시각화(Visualization)와 차원 축소(Dimension deduction),
연관규칙학습(Association rule learning) 등
- 군집화(Clustering)
•비지도 학습의 대표적인 기술로 x에 대한 레이블이 지정되어 있지 않은
데이터를 그룹핑하는 분석 알고리즘
-군집화(Clustering) : k-평균(k-Means)
주어진 데이터를 지정된 K개만큼의 그룹핑.
한 클러스터 내의 모든 데이터들은
다른 센트로이드보다 자신의 센트로이드와의 거리가 더 가깝다.
-군집화(Clustering) : k-평균(k-Means)
한 클러스터 내의 데이터들은 동일한 성질을 가지며 다른 그룹에 대하여 구별되는
군집을 찾아간다. 센트로이드(centroid)라 불리는 K개의 포인트로서
서로 다른 그룹내의 거리 평균값 중심(K-means)을 찾아간다.
이 때 데이터들은 K 클러스터 중 하나에만 속할 수 있다.
예시) 온라인 쇼핑몰의 고객들의 구매 패턴을 분석
고객 데이터(각 고객의 구매 금액과 구매 빈도 등의 정보),
K-means를 실행하면, 알고리즘은 고객들을 K개의 그룹으로 분류.
각 클러스터는 비슷한 구매 패턴을 가진 고객들을 포함하게 된.
생성된 클러스터를 분석하여 고객들의 선호도나 구매 경향을 파악할 수 있다.
 K-Means 클러스터링 이론 요약 
1. 개념
K-Means는 비지도 학습의 한 방법으로, 데이터를 K개의 그룹(클러스터)로 나누는 알고리즘입니다.
각 그룹은 중심점(centroid)을 기준으로 구성됩니다. 
2. 알고리즘 절차
초기화: 데이터 중 K개의 임의의 중심점을 선택합니다.
데이터 할당: 각 데이터를 가장 가까운 중심점에 할당합니다.
중심점 재계산: 각 클러스터에 속한 데이터들의 평균으로 새로운 중심점을 계산합니다.
반복: 중심점이 더 이상 바뀌지 않을 때까지 2~3단계를 반복합니다.
3. 특징
중심점과 각 데이터 간의 거리(보통 유클리드 거리)를 기준으로 군집화합니다.
클러스터 내 데이터들이 서로 비슷하고, 클러스터 간에는 차이가 크도록 구성됩니다. 
4. 장점
구현이 간단하고 계산 속도가 빠릅니다.
대규모 데이터셋에도 효율적으로 작동합니다. 
5. 단점
군집 수 K를 미리 정해야 합니다.
초기 중심점에 따라 결과가 달라질 수 있습니다.
구형 형태의 클러스터에 적합하며, 복잡한 분포에는 부적합할 수 있습니다. 
6. 활용 예시
고객 세분화
이미지 압축
뉴스/문서 분류

KNN vs K means ???
- k-최근접 이웃(k-Nearest Neighbors. k-NN) 지도학습
새로운 데이터 포인트에 대해 K개의 가장 가까운 데이터(Nearst Neighbors)를
찾아서 그 속성을 최다 빈도수의 feature로 결정.
예시) K=4,빨간 삼각형이 3개이므로 빨간 삼각형 집단 결정
- k-평균(k-Means) 비지도학습
주어진 데이터를 지정된 클러스터 갯수(k)로 그룹핑한다.
센트로이드(centroid)라 불리는 K개의 포인트로서
서로 다른 그룹내의 거리 평균값 중심(K-means)을 찾아간다.이 때 데이터들은 K
클러스터 중 하나에만 속할 수 있다. K개의 적정한 군집을 찾아가는 것이 목적

연관 분석은 데이터에서 항목들 간의 의미 있는 연관 관계를 발견하기 위한 비지도 학습 기법으로, 주로 장바구니 분석이나 추천 시스템 등에 활용된다. 연관 분석에서 자주 사용되는 대표적인 알고리즘은 Apriori이며, Python에서는 `mlxtend` 라이브러리를 이용해 쉽게 구현할 수 있다.

먼저 거래 데이터를 준비한 후, 이를 알고리즘에 적합한 형태로 변환하는 과정이 필요하다. 이때 `mlxtend.preprocessing` 모듈의 `TransactionEncoder` 클래스를 이용해 리스트 형태의 거래 데이터를 원-핫 인코딩 형식의 데이터프레임으로 바꾼다. 이 데이터는 각 항목이 열(column)로, 각 거래가 행(row)으로 구성되며, 항목이 존재하면 True, 없으면 False로 표시된다.

이후 `mlxtend.frequent_patterns` 모듈의 `apriori` 함수를 사용해 최소 지지도(min\_support)를 만족하는 빈발 항목 집합을 구한다. `use_colnames=True` 옵션을 주면 항목 이름이 그대로 출력된다. 이렇게 구한 항목 집합을 바탕으로 `association_rules` 함수를 이용하면 연관 규칙들을 추출할 수 있다. 이때 metric으로는 'confidence' 또는 'lift' 등을 설정할 수 있고, `min_threshold`를 통해 최소 기준값을 지정할 수 있다.

전체적인 사용 절차를 예시로 정리하면 다음과 같다.

1. 거래 데이터를 리스트 형태로 작성한다. 예: `[["bread", "milk"], ["bread", "jam", "milk"]]`
2. TransactionEncoder로 데이터를 원-핫 인코딩한다.
3. Pandas DataFrame으로 변환한다.
4. apriori 함수를 이용해 빈발 항목 집합을 찾는다.
5. association\_rules 함수를 이용해 연관 규칙을 도출한다.

이러한 과정을 통해 얻은 결과는 support(지지도), confidence(신뢰도), lift(향상도) 등의 지표와 함께 규칙 형태로 제공되며, 사용자는 이를 분석해 어떤 항목 조합이 의미 있는 관계를 가지는지 파악할 수 있다. 연관 분석은 구매 경향 파악, 마케팅 전략 수립, 상품 추천 등에 다양하게 활용된다.


-이상치 탐지(anomaly detection)
Normal(정상) 과 Abnormal(비정상, 이상치, 특이치) 을 구별해내는 문제를 의미
예시) 정상적인 범위내의 거래를 넘어서는 특이한 신용카드 거래내역을 찾아내서
부정/이상한 신용카드 거래를 감지 등

• Isolation Forest: 랜덤 포레스트와 비슷한 알고리즘으로, 이상치가 고립되기 쉬운
특성을 이용합니다.
• One-Class SVM: Support Vector Machine을 사용하여 정상 데이터를 정의하고,
정상 데이터와 다른 데이터를 이상치로 탐지합니다.

금융 거래 데이터에서 정상 거래와 현저히 다른 거래
사기 거래:설명: 한 사용자가 평소보다 훨씬 큰 금액을 갑자기 거래하거나, 거래 빈도와
패턴이 급격히 변화, 추가 확인 절차를 통해 거래의 정당성을 검증하거나, 사용자에게
알림을 보내고 거래를 일시적으로 차단할 수 있습니다.
사용자 오류: 잘못된 금액 입력이나 실수로 인해 비정상적으로 큰 금액이 입력
거래 확인 절차를 통해 오류를 수정하고, 사용자에게 알림을 보내 재확인.
특이한 이벤트:설명: 블랙 프라이데이와 같은 쇼핑 이벤트
이상치 탐지 모델을 조정하거나, 특정 기간 동안의 거래 패턴을 별도로 분석

금융 거래 데이터를 기반으로 이상치를 탐지하는 예시
Isolation Forest: 고립시키기 쉬운 이상치를 탐지하는 알고리즘.
Local Outlier Factor (LOF):
주변 데이터와의 밀도 차이를 이용하여 이상치를 탐지하는 알고리즘.

차원의 확대로 생기는 추가적인 차원은
기존데이터의 특성값(거리 등 상관관계)를 충분히 유지하며
위 예시처럼 제곱의 합 등으로 생성할 수 있다.

t-분포
정규분포는 데이터가 많을 때 신뢰도가 높습니다.
하지만 데이터가 적을 때(표본 수가 작을 때)는 정규분포를 그대로 쓰기엔
불확실성이 큽니다.그래서 등장한 게 t-분포입니다.
→ 정규분포보다 양쪽 꼬리가 더 두꺼워서, 작은 표본에서도 신뢰 구간을 더 넓게 잡습니다

t-SNE(t-Distributed Stochastic Neighbor Embedding) 차원 축소
고차원에서 이웃이었던 데이터를 2차원 또는 3차원으로 옮겨올 때
서로 가깝게 보존되도록 해주는 알고리즘. 차원 축소
t-SNE는 데이터 포인트 간의 거리를 보존하면서 시각화를 수행하므로,
시각화 결과에서 군집 간의 구분이 잘 되고 밀집도가 높은 영역에 군집이
모여있는 경향을 보인다.
기존 알고리즘(예: SNE)은 정규분포를 써서
"멀리 떨어진 데이터도 가까워 보이는 문제"가 있었습니다.
t-SNE는 꼬리가 두꺼운 t-분포를 써서,멀리 있는 점들이 너무 가까워지지 않도록 방지합니다.

PCA(Principal Component Analysis, 주성분 분석)
데이터의 주성분을 추출하는 비지도 학습 알고리즘.
주로 데이터의 차원 축소를 위해 사용되며,
데이터의 분산을 최대로 보존하는 축을 찾아
데이터를 새로운 저차원 공간으로 투영.
PCA는 주성분 벡터를 계산하여 데이터를 이루는 주요한 패턴을 파악하고, 이를
이용하여 데이터의 차원을 줄이는데 사용.
주성분은 데이터의 분산을 가장 잘 설명하는 축으로, 이 축들로 데이터를
투영함으로써 원래 차원의 정보를 가능한 한 보존하면서도 차원을 축소할 수
있다.

선형 vs 비선형: PCA는 선형 기법, t-SNE는 비선형 기법
전역 vs 국소 구조:
PCA는 데이터의 전역 구조를 보존하려 하고, t-SNE는 국소 구조를 보존한다.
PCA는 전체적인 데이터의 구조를 간단히 요약하는 데 유용하고,
(국영수 과목의 차원축소 등)
t-SNE는 복잡한 데이터의 군집 구조를 시각화하는 데 효과적
( 다양한 동물사진의 데이터 유사성 분석 등)

pca 공간의 축소떄문에 작아보임
tsne는 너무 퍼짐

비지도 학습은 정답이 없는 데이터를 가지고 **패턴이나 숨겨진 구조를 찾는 방법**이다. 이 중에서 많이 쓰이는 두 가지는 **차원 축소**와 **이상치 탐지**이다.

**차원 축소**는 데이터에 포함된 여러 가지 정보(예: 수십 개의 특징) 중에서 **중요한 것만 골라서 줄이는 방법**이다. 이렇게 하면 데이터를 보기 쉽게 만들 수 있고, 계산도 더 빨라진다. 대표적인 방법으로는 PCA, t-SNE, UMAP, 오토인코더가 있다. 예를 들어 PCA는 데이터의 가장 큰 변화를 따라 새로운 축을 만들고, t-SNE나 UMAP은 복잡한 구조를 2차원이나 3차원으로 줄여서 시각화할 때 자주 사용된다.

**이상치 탐지**는 대부분의 데이터와 다른 **특이한 데이터**를 찾아내는 것이다. 예를 들어 정상적인 거래 데이터 속에서 사기 거래를 찾는 데 쓴다. 이상치를 찾는 방법에는 여러 가지가 있다. K-NN이나 LOF는 **데이터 사이의 거리나 밀도**를 기준으로 판단하고, Isolation Forest나 One-Class SVM 같은 **모델을 이용한 방법**도 있다. 간단한 통계 방법으로 평균에서 너무 벗어난 값을 이상치로 보는 방법도 있다.

차원 축소와 이상치 탐지는 같이 쓰일 수도 있다. 예를 들어 데이터를 2차원으로 줄여서 눈으로 보고 이상치를 쉽게 찾을 수 있다. 두 방법 모두 데이터를 이해하고 분석하는 데 아주 유용한 도구다.

