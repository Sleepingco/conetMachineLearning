# -*- coding: utf-8 -*-
"""0512ML

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hptba2jek6uX3Uw0W5LRXZcoQV2G80rx
"""

from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
import pandas as pd

# 데이터 로드
data  = load_breast_cancer()
X,y = data.data, data.target

# DataFrame 생성
df = pd.DataFrame(X, columns=data.feature_names)
df['target'] = y

# 확인
print(data.DESCR)  # 데이터셋 전체 설명

# 학습/테스트 데이터 분할
X_train,X_test, y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=42)

# 로지스틱 회귀 모델 생성
model = LogisticRegression(max_iter=10000)

# 모델 학습
model.fit(X_train,y_train)

# 예측
y_pred = model.predict(X_test)

# 정확도 계산
accuracy =  accuracy_score(y_test, y_pred)
print(f'Accuracy:{accuracy:.4f}')

from sklearn.linear_model import LogisticRegression
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split

# Iris 데이터셋 로드(클래스 2 제외)
iris = load_iris()
X,y=iris.data, iris.target
X,y=X[y!=2],y[y!=2] # 2진 분류로 변환

# 학습 데이터와 데스트 데이터 분할
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=42)

# 로지스틱 회귀 모델 생성
model = LogisticRegression()

# 학습
model.fit(X_train,y_train)

# 예측
y_pred = model.predict(X_test)

# 정확도 계산
accuracy = model.score(X_test, y_test)
print(f'Accuracy:{accuracy:.4f}')

from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.multiclass import OneVsRestClassifier
from sklearn.metrics import accuracy_score

# 데이터 로드
iris = load_iris()
X, y = iris.data, iris.target

# 학습/테스트 데이터 분할
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 로지스틱 회귀 모델 생성
model = LogisticRegression(max_iter=200)

# OneVsRestClassifier 사용하여 다중 클래스 분류기 생성
ovr_model = OneVsRestClassifier(model)

# 모델 학습
ovr_model.fit(X_train, y_train)

# 예측
y_pred = ovr_model.predict(X_test)

# 정확도 계산
accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy: {accuracy:.4f}')

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import make_pipeline
from sklearn.datasets import load_breast_cancer

# 데이터를 로드하고 전처리(학습데이터의 표준화)
X_train,y_train = load_breast_cancer(return_X_y=True)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)

# 모델과 파라미터 그리드를 정의
model =  LogisticRegression()
param_grid = {'C':[0.001, 0.01, 0.1, 1, 10, 100],'penalty':['l1','l2']}

# 그리드 서치를 수행.
grid_search = GridSearchCV(model, param_grid, cv=5)
grid_search.fit(X_train_scaled,y_train)

# 최적 파라미터와 최고 정확도를 출력.
print("Best Hyperparameters",grid_search.best_params_)
print("Best Accuracy",grid_search.best_score_)

# 테스트 데이터에 대해 모델을 평가.
X_test, y_test = load_breast_cancer(return_X_y=True)
X_test_scaled = scaler.transform(X_test) # 테스트 입력 데이터 표준화
test_accuracy = grid_search.score(X_test_scaled,y_test)
print("Test Accuracy",test_accuracy)

model = LogisticRegression(C=1,penalty='l2')
model.fit(X_train_scaled,y_train)

# 테스트 데이터로 예측 수행
y_pred = model.predict(X_test_scaled)
# 정확도 평가
accuracy = accuracy_score(y_test,y_pred)
print("Accuracy",accuracy)

import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report

# 혼동 행렬 계산
cm = confusion_matrix(y_test, y_pred)

# 혼동 행렬 출력
print("Confusion Matrix:")
print(cm)

# 혼동 행렬 시각화
plt.figure(figsize=(6, 4))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=data.target_names, yticklabels=data.target_names)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

# 분류 리포트 출력
print("\nClassification Report:")
print(classification_report(y_test, y_pred, target_names=data.target_names))

from sklearn. linear_model import LinearRegression
from sklearn.model_selection import train_test_split
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

"""데이터 로드 :데이터셋을 로드하여 분석에 활용할 수 있도록 준비.
fetch_california_housing()
housing_df = pd.DataFrame(housing.data, columns = housing.feature_names)
데이터 탐색(EDA):데이터셋의 구성, 변수의 분포, 변수들 간의 상관 관계 등을 탐색. 시각화 도구
housing_df.head(), housing_df.describe() sns.pairplot(housing_df)
데이터 전처리:학습에 적합한 형태로 가공. 변수 스케일링, 결측치 처리, 범주형 변수 인코딩 등.
StandardScaler()
모델 학습:주택 가격 예측 모델을 선택. 선형 회귀, 의사결정 트리, 랜덤 포레스트 등 다양한 모델.
model = LinearRegression() model.fit(X_train, y_train)
모델 평가:학습된 모델의 성능을 평가. MSE, 결정 계수(R-squared) 등을 사용.
model.score(X_test, y_test)
모델 튜닝:모델의 성능을 향상시키기 위해 하이퍼파라미터를 조정, 교차 검증 등.
cross_val_score(model, housing.data, housing.target, cv=10, scoring='r2')
결과 해석:모델의 결과를 해석하고 주택 가격에 영향을 미치는 주요 요소를 식별.
"""

from sklearn.datasets import fetch_california_housing
housing = fetch_california_housing()

print(housing.keys())
print(housing.DESCR)

housing_df = pd.DataFrame(housing.data, columns=housing.feature_names)
print(housing.target)
housing_df['Price'] = housing.target
housing_df.head()

housing_df.describe()

for i, col in enumerate(housing_df.columns): # 열의 인덱스와 값을 동히게 가져온다
  plt.figure(figsize=(8,4))
  plt.plot(housing_df[col]) # 해당 열의 데이터로 그리기
  plt.title(col)            # 해당 열로 제목 쓰기
  plt.xlabel('Location')    # 임의의 위치
  plt.tight_layout()

for i, col in enumerate(housing_df.columns):
  plt.figure(figsize=(8,4))
  plt.scatter(housing_df[col],housing_df['Price']) # 집값에 대한 열 데이터의 산포도
  plt.title(col)
  plt.ylabel('Price',size=12)
  plt.xlabel(col,size=12)
  plt.tight_layout()

import seaborn as sns

sns.pairplot(housing_df);

housing_df.plot(kind='scatter',x='Longitude',y='Latitude',alpha=0.2,figsize=(12,10));
plt.legend(['Housing location']) # 위도 경도에 다른 밀집도

housing_df.plot(kind='scatter',x='Longitude',y='Latitude',alpha=0.2,s=housing_df['Population']/100,label='Population'
,figsize=(15,10),c='Price',cmap=plt.get_cmap('viridis'),colorbar=True)
plt.legend(['Housing loaction'])

model = LinearRegression()

import sklearn
print(sklearn.__version__)

from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test=train_test_split(housing.data,housing.target,test_size=0.3)

model.fit(X_train,y_train)
model.score(X_test,y_test)

print("training Data evaluation:{}".format(model.score(X_train,y_train)))
print("test Data evaluation:{}".format(model.score(X_test,y_test)))

from sklearn.model_selection import cross_val_score
scores = cross_val_score(model, housing.data,housing.target,cv=10,scoring='r2')
print('결정계수= {}'.format(scores))
# 결정계수는 1에 가가울수로 모델을 잘 설명한다. 다중선형 분석이라 다수의 결정계수

print('y ='+str(model.intercept_)+' ')
for i,c in enumerate(model.coef_):
  print(str(c)+' *X'+str(i))

from sklearn.metrics import mean_squared_error, r2_score

y_train_predict = model.predict(X_train)
rmse = (np.sqrt(mean_squared_error(y_train,y_train_predict)))
r2 = r2_score(y_train,y_train_predict)

print('rmse : {}'.format(rmse))
print('R2 score" {}'.format(r2))

from sklearn.metrics import mean_squared_error, r2_score

y_test_predict = model.predict(X_test)
rmse = (np.sqrt(mean_squared_error(y_test,y_test_predict)))
r2 = r2_score(y_test,y_train_predict)

print('rmse : {}'.format(rmse))
print('R2 score" {}'.format(r2))

def plot_housing_prices(expected, predicted):
  plt.figure(figsize=(8,4))
  plt.scatter(expected,predicted)
  plt.plot([0.14,5.1],[0.14,5.1],'--b')
  plt.xlabel('True price($100,000s)')
  plt.ylabel('predicted price($100,000s)')
  plt.tight_layout()

predicted = model.predict(X_test)
excepted = y_test

plot_housing_prices(excepted,predicted)
print(excepted,predicted)

# Standardize/Normalize
from sklearn.linear_model import LinearRegression
model = LinearRegression()

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
X_train, X_test, y_train, y_test = train_test_split(housing.data, housing.target, test_size=0.3)

import pandas as pd

X_test_df = pd.DataFrame(X_test)
pd.set_option('display.float_format', '{:.2f}'.format) #소수점 둘째 자리까지 표시
X_test_df.describe()

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.fit_transform(X_test)

"""데이터의 표준화를 수행
StandardScaler() : 객체를 생성하여 변수 scaler에 할당.
fit_transform() 메서드를 사용하여 훈련 데이터셋인 X_train을 표준화
훈련 데이터셋의 평균과 표준편차를 계산하고, 각 데이터 포인트를 평균으로부터의
거리에 대한 표준편차로 나누어 표준화된 값을 얻는다.
동일한 StandardScaler() 객체를 사용하여 테스트 데이터셋인 X_test도 표준화.
훈련 데이터셋의 평균과 표준편차를 사용하여 테스트 데이터셋을 표준화.
표준화된 데이터를 사용하면 각 특성의 값이 평균 0, 표준편차 1을 가지게 되어서
다른 특성들과 비교하기 쉬워지고, 머신러닝 모델 학습에 도움
"""

import pandas as pd
# 평균이 0 이고 표준편차가 1인 표준 정규분포로 변형
x_test_df = pd.DataFrame(X_test)
pd.set_option('display.float_format','{:.2f}'.format)
X_test_df.describe()

model.fit(X_train,y_train)
model.score(X_test,y_test)

print("training Data evaluation : {}".format(model.score(X_train, y_train)))
print("test Data evaluation : {}".format(model.score(X_test, y_test)))
# score : 주어진 데이터에 대한 모델의 예측 정확성이나 설명력을 측정

"""Before Standardized :
training Data evaluation:0.6046874249253127
test Data evaluation:0.6092018243200261
"""

# 상관 행렬 계산
correlation_matrix = pd.DataFrame(np.corrcoef(housing.data.T),columns=housing.feature_names,index=housing.feature_names)

# 상관 행렬 출력
print(correlation_matrix)

"""correlation_matrix = pd.DataFrame(np.corrcoef(housing.data.T),columns=housing.feature_names,index=housing.feature_names)

특성들 간의 상관관계를 계산하여 상관계수(correlation coefficient) 행렬을 생성
np.corrcoef() 함수는 주어진 데이터의 상관계수를 계산하는 함수.
housing.data.T : 특성들을 전치(transpose)하여 각 특성이 열로 표현.
np.corrcoef(housing.data.T) : 전치된 데이터셋의 특성들 간의 상관계수 행렬을 계산.
pd.DataFrame()함수를 사용하여 계산된 상관계수 행렬을 판다스 데이터프레임으로 변환.
columns=housing.feature_names와 index=housing.feature_names는 열과 행의 이름을 주어진
데이터셋의 특성 이름으로 설정.
correlation_matrix는 데이터셋의 특성들 간의 상관계수를 나타내는 행렬로서,
 각 특성들 간의 상관관계를 파악
"""

# Ridge Regression
from sklearn.linear_model import Ridge
import pandas as pd

from sklearn.datasets import fetch_california_housing
housing = fetch_california_housing()

housing_df = pd.DataFrame(housing.data,columns=housing.feature_names)
housing_df['Price'] = housing.target
housing_df.head()

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(housing.data, housing.target, test_size=0.2)

model = Ridge(alpha=10) # 0.1, 1, 10 정도의 알파값을 추천
model.fit(X_train, y_train)

print("training Data evaluation : {}".format(model.score(X_train,y_train)))
print("test Data evaluation : {}".format(model.score(X_test, y_test)))
# score : 주어진 데이터에 대한 모델의 예측 정확성이나 설명력을 측정

predicted = model.predict (X_test)
expected = y_test
plot_housing_prices(expected, predicted)

# Lasso Regression
from sklearn. linear_model import Lasso
import pandas as pd

from sklearn.datasets import fetch_california_housing
housing = fetch_california_housing()
housing_df = pd.DataFrame(housing.data, columns = housing.feature_names)
housing_df['Price'] = housing.target
housing_df.head()

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(housing.data, housing.target, test_size=0.2)

model = Lasso(alpha = 0.001)
model.fit(X_train,y_train)

print('training Data evaluation : {}'.format(model.score(X_train,y_train)))
print('test Data evaluaion : {}'.format(model.score(X_test,y_test)))

from sklearn.linear_model import ElasticNet
from sklearn.datasets import fetch_california_housing
housing = fetch_california_housing()

housing_df = pd.DataFrame(housing.data, columns = housing. feature_names)
housing_df['Price'] = housing.target
housing_df.head()

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(housing.data, housing.target, test_size=0.2)

model = ElasticNet(alpha = 0.005,l1_ratio=0.05) # L1, L2의 가중치 설정
model.fit(X_train, y_train)

print('training Data evaluation : {}'.format(model.score(X_train,y_train)))
print('test Data evaluaion : {}'.format(model.score(X_test,y_test)))

from sklearn.linear_model import Lasso, Ridge, ElasticNet
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import r2_score

from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split
import pandas as pd

# 데이터셋 로드
data = fetch_california_housing()
X, y = data.data, data.target

# 훈련/테스트 분리
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

from sklearn.linear_model import Lasso

param_grid = {'alpha': [0.001, 0.01, 0.1, 1, 10, 1000]}
lasso = Lasso(max_iter=10000)
grid = GridSearchCV(lasso, param_grid, cv=5)
grid.fit(X_train, y_train)

print("Lasso Best alpha:", grid.best_params_['alpha'])
print("Lasso Best R² (train CV):", grid.best_score_)

# 테스트 세트에서 R²
y_pred = grid.predict(X_test)
print("Lasso R² on test set:", r2_score(y_test, y_pred))

from sklearn.linear_model import Ridge

param_grid = {'alpha': [0.001, 0.01, 0.1, 1, 10, 1000]}
ridge = Ridge()
grid = GridSearchCV(ridge, param_grid, cv=5)
grid.fit(X_train, y_train)

print("Ridge Best alpha:", grid.best_params_['alpha'])
print("Ridge Best R² (train CV):", grid.best_score_)

y_pred = grid.predict(X_test)
print("Ridge R² on test set:", r2_score(y_test, y_pred))

from sklearn.linear_model import ElasticNet

param_grid = {
    'alpha': [0.001, 0.01, 0.1, 1, 100,1000],
    'l1_ratio': [0.1, 0.5, 0.9]  # L1:L2 혼합 비율
}
elastic = ElasticNet(max_iter=10000)
grid = GridSearchCV(elastic, param_grid, cv=5)
grid.fit(X_train, y_train)

print("ElasticNet Best params:", grid.best_params_)
print("ElasticNet Best R² (train CV):", grid.best_score_)

y_pred = grid.predict(X_test)
print("ElasticNet R² on test set:", r2_score(y_test, y_pred))

