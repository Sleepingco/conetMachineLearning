강화학습(Reinforcement Learning)
지도 학습과 비지도 학습이
환경에 변화가 없는 정적인 환경의 데이터셋을 기반으로 학습을 진행했다면,
강화 학습은 동적인 환경에서의 학습 !
강화 학습은
어떤 환경 안에서 정의된 주체(agent)가
현재의 상태(state)를 관찰하여 선택할 수 있는 행동(action)들 중에서
가장 최대의 보상(reward)을 가져다 주는 행동이 무엇인지를 학습하는 것

1. 입력 형태 (Input Shape)
DQN의 입력은 주로 게임 화면(프레임).
DQN은 화면의 픽셀 데이터를 처리
프레임 전처리:원본 화면은 RGB 이미지(210x160x3)로 제공,
계산 효율성을 위해 전처리
• 그레이스케일 변환: RGB를 그레이스케일로 변환하여 채널 수를 줄임(3 → 1).
• 다운샘플링: 이미지 크기를 줄임(예: 210x160 → 84x84).
• 정규화: 픽셀 값을 [0, 255]에서 [0, 1]로 정규화.
• 프레임 스태킹(Frame Stacking): 에이전트가 공의 속도와 방향(운동 정보)을
인식하려면 연속된 프레임이 필요합니다.
• 4개의 연속 프레임을 스택하여 (84x84x4) 형태의 입력을 만듭니다. >> 최종 입력
형태: (84, 84, 4) (높이, 너비, 채널).
추가 전처리:배경과 같은 불필요한 정보를 제거하기 위해 픽셀 차이(프레임 간 차분) 계산.
상단 점수 표시 부분(예: 007, 5, 1) 제거.

2. 출력 형태 (Output Shape)
DQN의 출력은 각 행동에 대한 Q-값(Q-value)입니다.
Breakout 게임에서 가능한 행동은 패들의 움직임과 관련된 이산 행동(discrete actions)
행동 집합:Breakout 게임에서는 일반적으로 4가지 행동이 사용:
• NOOP (아무 행동 안 함)
• FIRE (공 발사, 게임 시작 시 필요)
• LEFT (패들을 왼쪽으로 이동)
• RIGHT (패들을 오른쪽으로 이동)
출력은 4개의 행동에 대한 Q-값을 나타내는 벡터
출력 형태: (4,) (행동 개수만큼의 Q-값).
DQN은 이 Q-값 중 가장 높은 값을 가진 행동을 선택하거나,
탐험(exploration)을 위해 ε-greedy 정책에 따라 무작위 행동을 선택합니다.

3. 기본 아키텍처 (DQN Architecture)
Breakout과 같은 Atari 게임을 해결하기 위해 DQN은 주로 Convolutional Neural
Network(CNN) 기반 아키텍처를 사용
입력: (84, 84, 4)
→ Conv(32, 8x8, stride 4) → ReLU → (20, 20, 32)
→ Conv(64, 4x4, stride 2) → ReLU → (9, 9, 64)
→ Conv(64, 3x3, stride 1) → ReLU → (7, 7, 64)
→ Flatten → 3136
→ Dense(512) → ReLU
→ Dense(4) → 출력(Q-값)

4.추가 고려사항
보상 함수:
Breakout에서는 블록을 깰 때마다 양의 보상(예: +1),
공을 놓치면 음의 보상(예: -1),
게임 종료 시 큰 음의 보상(예: -10) 등을 설정.
탐험 전략:
ε-greedy 정책 사용: 초기에는 탐험을 많이 하고(ε=1), 점차 줄임(ε → 0.1).
경험 재생(Experience Replay):과거 경험(상태, 행동, 보상, 다음 상태)을 저장하고,
랜덤 샘플링하여 학습.버퍼 크기: 약 1,000,000 샘플.
타겟 네트워크 :
메인 네트워크의 복사본으로, Q-값 계산 시 안정적인 기준을 제공합니다.
메인 네트워크는 계속 학습하고, 타겟 네트워크는 일정 주기마다 업데이트되어
학습 과정의 흔들림을 줄입니다. 이를 통해 DQN이 Breakout 같은 게임에서 더 잘
학습할 수 있습니다!
DQN의 기본 문제: DQN은 Q-값을 학습하면서 신경망이 계속 업데이트됩니다. 그런데 Q-값을 계산할 때, 같은
네트워크를 사용해서 현재 상태의 Q-값과 다음 상태의 Q-값(타겟 Q-값)을 동시에 추정하면, 네트워크가 "자기
자신을 쫓아가는" 상황이 됩니다. 이 과정에서 학습이 불안정해지고, Q-값이 발산하거나 진동할 수 있습니다.
4.추가 고려사항
손실 함수:
DQN은 Q-러닝을 기반으로 하므로,
손실 함수는 MSE(Mean Squared Error)를 사용합니다.
타겟 Q-값은 벨만 방정식(Bellman equation)을 통해 계산됩니다:

데이비드 베이커 연구팀은 알파폴드에 기반해 로제타폴드 프로그램을 만들어 아미노산의
서열을 찾아냈다. 이를 바탕으로 실제 자연에서 발견되지 않은 새로운 단백질 구조를
설계하고 만드는데 성공했다. 이들 연구팀은 코로나19 바이러스의 단백질 구조를 확인하며
질병 억제제 개발에 기여했다. 또 펜타닐을 감지할 수 있는 센서도 설계했다.
노벨상위원회는 알파폴드와 로제타폴드 프로그램으로 생화학분야의 획기적인 성과를
가져올 수 있다고 기대했다.
출처 : 헬로디디(http://www.hellodd.com)

③ 강화학습(Reinforcement Learning)
강화학습은 에이전트가 환경과 상호작용하며,
보상을 최대화할 수 있는 정책(행동 전략)을 MDP 가정 하에 학습하는 프레임워크
1.정의된 주체(agent)가 주어진 환경(environment)의 현재 상태(state)를
관찰(observation)하여, 이를 기반으로 행동(action)을 취한다
2.이때 환경의 상태가 변화하면서 정의된 주체는 보상(reward)을 받게 된다.
3.이 보상을 기반으로 정의된 주체는 더 많은 보상을 얻을 수 있는 방향(best
action)으로 행동을 학습하게 된다

⚫ 마르코프 결정 프로세스(Markov Decision Process, MDP)
에이전트가 환경에서 최적의 행동을 선택하는 과정을 수학적으로 모델링한 프레임워크
일련의 상태와 확률적인 상태 전이에 기반하여 의사 결정 문제를 모델링.
시간에 따라 변화하는 상황에서 최적의 의사 결정을 내릴 수 있다

MDP의 구성 요소 :
1.상태(State): 모델이 의사 결정을 내릴 때 고려해야 하는 정보를 포함.
2.행동(Action): 각 상태에서 가능한 행동의 집합.
3.보상(Reward): 어떤 행동을 선택시때 받는 즉시적인 보상. 목표는 누적 보상을 최대화.
4.상태 전이 확률(State Transition Probability): 각 상태에서 특정 행동을 선택했을 때,
다음 상태로 전이될 확률.
5.할인율(Discount Factor): 미래의 보상을 현재보다 더 낮은 가치로 고려하기 위해
사용되는 요소. 0과 1 사이의 값. 0에 가까울수록 미래의 보상에 더 적은 가치를 부여.

구분 /MDP/ Q-Learning
정의/ 환경의 규칙을 정의하는 설계도/ 규칙 없이 학습하는 탐험
알고리즘/ 미로의 설계도 (방, 길, 보물, 확률/) 지도 없이 탐험으로 경로 찾기
모델/ 여부 전이 확률, 보상 함수 포함/ 모델 프리 (경험으로 학습)
역할/ 규칙 정의 및 계산 기반 제공/ 시행착오로 최적 정책 학습

주요 강화 학습 알고리즘
Q-Learning: 상태-행동 쌍의 Q-값을 업데이트하여 최적의 정책을 학습하는 방법
DQN (Deep Q-Network): 심층 신경망을 사용하여 Q-값을 근사하는 방법.
SARSA (State-Action-Reward-State-Action): Q-Learning과 유사하지만, 다음
상태에서의 행동을 고려하여 Q-값을 업데이트합니다.
Policy Gradient Methods: 정책을 직접 학습하는 방법으로, 대표적으로 REINFORCE
알고리즘이 있습니다.
Actor-Critic Methods: 정책 학습(Actor)과 가치 함수 학습(Critic)을 결합한 방법.

주요 기술 트렌드
심층 강화 학습(Deep Reinforcement Learning):심층 신경망을 활용하여 고차원
상태 공간을 처리하고, 복잡한 정책을 학습.
대표적인 알고리즘으로 DQN, PPO(Proximal Policy Optimization) 등이 있습니다.
모델 기반 강화 학습(Model-Based Reinforcement Learning):환경의 동작을
모델링하여 에이전트가 시뮬레이션을 통해 학습.
환경 모델을 사용하여 미래 상태와 보상을 예측하고, 샘플 효율성을 높입니다.
다중 에이전트 강화 학습(Multi-Agent Reinforcement Learning):
여러 에이전트가 동시에 학습하고 협력하거나 경쟁하는 상황을 다룹니다.
예를 들어, 다중 로봇 제어, 게임 플레이, 자율 주행 차량 간의 상호작용 등.
전이 학습(Transfer Learning) 및 메타 학습(Meta-Learning):
학습한 정책이나 지식을 다른 유사한 환경이나 작업으로 전이하여 학습 효율성을
높입니다.

gymnasium frozen lake
4x4 그리드에서 0,0 to goal 도착이 목적
observation: 다음 행동을 했을때에 내 상태 frozen or hole, goal
state 격자판의 상태
행동 좌하우상0123
reward :g 에 도달시 1 , hole 0, frozen
terminated 종료
🎯 목표: (0, 0) → goal 도착  
👀 observation: 다음 행동을 했을 때의 내 상태 → frozen, hole, goal  
📍 state: frozen, hole, goal  
🎮 행동: 0=좌, 1=하, 2=우, 3=상  
💰 reward: goal→1, hole→0, frozen  
🛑 terminated: hole 또는 goal 도달 시  
🔁 step: obs, reward, terminated, truncated, info = env.step(action)  
➡️ step 구성: obs=다음 상태, reward=보상, terminated=종료 여부, truncated=시간초과 종료(step 200), info=추가 정보
