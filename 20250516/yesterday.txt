DecisionTree(과적합의 위험성이 강해서 RandomForest를 만듬) -> RandomForest(작은 모델 여러개) -> Bagging(1만개의 데이터에서 뽑는 방법 비복원 추출(복원 추출 방식은 BootStraping), 배깅은 단독적 모델 페러럴하게) -> Boosting(모델을 추출해서 학습이 잘되는 나무와 안되는 나무를 추출 학습을 잘못한 모델을 학습을 순차적으로 학습을함) -> GradiantBoosting
앙상블(RandomForest,Bagging,Boosting 등등이 앙상블 기법 여러 모델을 한번에 같이씀 동종 모델 혹은, 이종모델, Voting 기법도 비슷)
Voting(작은 모델을 모아서 과반수, 빈도수로 정함,이종모델을 사용하는 기법,빈도수화 확률적으로 채택(hard,soft))

손실함수
(1) 분류 문제 (예: 스팸 메일 분류)
손실함수: 지니 불순도(Gini Impurity)나 엔트로피(Entropy).
데이터를 나눴을 때, 스팸과 비스팸이 섞여 있으면 "혼란스럽다" → 손실이 큼.
한 그룹에 스팸만, 다른 그룹에 비스팸만 있으면 "깔끔하다" → 손실이 작음.
(2) 회귀 문제 (예: 집값 예측)
손실함수: 평균 제곱 오차(MSE, Mean Squared Error).
한 그룹의 집값들이 비슷하면 "예측이 정확하다" → 손실이 작음.
한 그룹에 비싼 집과 싼 집이 섞여 있으면 "예측이 부정확하다" → 손실이 큼

배깅 ex) 백만개 데이터를 한번에 뽑을수없기에 1만개씩 트리를 만들어서 그중에서 100개씩 뽑아서 DT를 만들고 다시 학습한 데이터도 1만개 데이터에 넣어서 다시 랜덤으로 뽑아 중복을 허용하고 DT를 생성하는 방식
중복되게 샘플을 추출하는 이유. (복원추출)
1.통계적 추론: 모집단에 대한 통계적인 추론을 수행할 수 있다.
2.모델 평가: 모델의 예측 성능을 평가, 모델의 일반화 성능을 추정 가능
3.파라미터 추정: 모델의 파라미터를 추정할 수 있다. 이를 통해 모델의 파라미터에
대한 불확실성을 평가할 수 있다.

의사결정 트리의 본래 모델은 트리를 만들 때 모든 속성들을 살펴보고
정보 획득량이 가장 많은 속성(중요한 질문)을 선택해서 데이터를 분할한다.
전체 속성들 중 일부만 랜덤하게 고려하여 트리를 작성하여 다수의 트리를
만들고 그 결과값이 나오면 voting/aggregating을 통해 최종 결과값을 결정

 Random selection of Features
예시) 30개의 속성(features)가 있다면 3개씩 임의로 features(질문)을 뽑아서
의사결정 트리를 구성하고 결과값을 추출해 낸다

부스팅 학습 결과가 좋지않은 분류기의 학습 데이터 샘플에 가중치를 부여해서 좀더 정확도 높은 모델을 만드는 방법

부스팅(Boosting):
Gradient Boosting: 이전 모델의 예측과 실제값의 차이인 잔차오류(residual
error)를 최소화하기 위해 새로운 모델을 학습시키는 방식으로 오차를 보완하는
앙상블 모델

스태킹(Stacking):
다양한 기본 모델들을 학습 후, 그 예측 결과를 기반으로 메타 모델을 학습시키는 방식.
기본 모델들의 예측 결과를 입력으로 사용하여 메타 모델을 학습하고 최종 예측을 수행.
메타 모델은 다양한 머신러닝 알고리즘을 사용할 수 있으며, 일반적으로는 선형
모델이나 신경망 모델 등을 사용
yolo의 결과를 xg부스트에 넣어서 결과를 만드는 방법을 연구중

Cross_validate에서 scoring 파라미터를 생략하면,
분류기(classifier)의 경우 정확도(accuracy)가,
회귀기(regressor)의 경우 𝑅2가 기본 성능 지표로 사용됩니다. 