1 지도학습
분류와 회귀 문제를 해결하는데 사용

주어진 데이터에 대해 가장 가까운 이웃 데이터의 레이블을 기반으로 예측을수행
k개의 가장 가까운 데이터를 찾아서 그 속성을 최다 빈도수의 특징(feature)으로 결정
작동원리
데이터 준비
이웃의 개수 선택
거리 측정 (이웃데이가와의 거리를 유클리드 거리를 사용, 입력 데이터의 특징들을 비교하여 거리를 계산)
이웃 데이터 선택
분류 또는 회귀 예측
instance based learning : knn 별도의 모델 생성없이 인접 데이터로 분류/예측
model based learning : 선형/비선형 regression, decision tree, nn, SVM

장점:
1. 간단하고 직관적인 알고리즘 이웃들의 특성을 활용하여 예측, 직관적.
다목적 사용: KNN은 분류와 회귀 문제 모두에 사용.
 분류에서는 이웃들의 다수결 투표를 통해 클래스를 할당하고,
 회귀에서는 이웃들의 평균값

 한계점
 계산 비용:KNN은 예측을 수행할 때 모든 훈련 데이터와의 거리를 계산. 훈련
데이터가 크고 차원이 높은 경우에는 계산 비용이 증가.
차원의 저주: 차원이 증가할수록 각 데이터 포인트 간의 거리가 멀어지는 현상인
차원의 저주가 발생. KNN의 성능이 저하될 수 있다.
데이터 불균형:이웃들의 다수결로 예측을 수행하기 때문에 데이터의 클래스
불균형이 있을 경우 예측이 편향될 수 있다. 이 경우 적은 클래스의 데이터는 제대로
고려되지 않을 수 있다.
weighted knn: 한계점을 극복하기 위한 시도
k값에 따르는 이웃들의 위치는 거리가 가까울 수도 멀 수도 있다.
거리만큼의 가중치를 고려하는 것이 합리적일 수 있다

leaf_size:트리의 리프 노드(leaf node)에 포함될 수 있는 최대 데이터 포인트 수
 탐색 속도를 최적화하기 위해 도입된 트리 기반 검색 구조의 하이퍼파라미터
너무 작으면 메모리와 계산량이 많아지고, 너무 크면 탐색이 느려질 수 있다.
p:Minkowski 거리에 사용할 파라미터.
 1은 맨해튼 거리, 2는 유클리디언 거리를 의미. 기본값은 2.
metric:거리 측정 방법을 지정. 기본값은 "minkowski".
 다른 값으로 "euclidean", "manhattan", "chebyshev" 등이 있다.
n_jobs: CPU의 병렬 처리를 사용하여 이웃 탐색을 가속화.
 -1로 설정하면 가능한 모든 프로세서를 사용. 기본값은 1.
 
 n_neighbors:분류에 사용할 이웃의 개수를 지정.
 일반적으로 홀수로 설정하여 다수결 원칙에 따라 분류결과를
 결정하는 데 사용. 기본값은 5.
weights:이웃들에 대한 가중치를 지정하는 방법을 선택.
 가능한 값으로는 "uniform" (거리에 관계없이 동일한 가중치, Iris),
＂distance＂ (거리에 반비례하는 가중치, house price) 및
 “callable” 사용자 정의 함수가 있다.
 기본값은 "uniform".
algorithm:이웃을 검색하는 알고리즘을 선택.
 "auto" (자동으로 최적을 선택), "ball_tree", "kd_tree", "brute“
작은 데이터셋에는 "brute"를 사용. 기본값은 "auto".

algorithm 파라미터
'auto':
fit 메서드에 전달된 값에 기반하여 가장 적절한 알고리즘을 자동으로 결정.
희소한 입력 데이터에 대해서는 brute-force search 알고리즘이 적용.
'ball_tree':유클리디언 거리를 측정방법으로 사용
데이터의 분산을 기준으로, 공간을 트리구조로 구성하여 이웃을 탐색하는 방법.
데이터셋이 대규모이고 차원이 높은 경우에 효율적인 알고리즘.
'kd_tree':맨해터 거리를 측정방법으로 사용
데이터의 특성축을 기준으로 트리 구조로 구성하여 이웃을 탐색하는 방법.
데이터셋이 중간 규모이고 차원이 중간 정도인 경우에 효율적.
'brute':
brute-force search는 모든 데이터 포인트 간의 거리를 계산하여 이웃을 찾는 방법.
데이터셋이 상대적으로 작거나 차원이 매우 낮은 경우에 사용.